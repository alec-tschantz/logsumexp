


\documentclass{article}
\usepackage{amsmath,amssymb,bm}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\begin{document}

\title{Attention via \(\log \sum \exp\) energy}
\author{Alexander Tschantz}
\date{\today}
\maketitle

\section{General Framework}

\paragraph{Setup.}  
We consider a single set of nodes \(\bm{v} = \{\bm{v}_i : i \in \{1, 2, \ldots, N\}\}\), where each node \(\bm{v}_i \in \mathbb{R}^{d_i}\). The relationships between these nodes are defined by a set of \(M\) energy functions \(\{E_m : m \in \{1, 2, \ldots, M\}\}\). Each energy function \(E_m\) defines a subset of nodes acting as \emph{children} \(C_m \subseteq \{1, 2, \ldots, normalization\}\) and a subset acting as \emph{parents} \(P_m \subseteq \{1, 2, \ldots, N\}\), which may overlap.

\paragraph{Energy.}  
Each energy function \(E_m\) defines a similarity function:
\begin{equation}
\mathrm{sim}\bigl(\bm{v}_c, \bm{v}_p\bigr) \quad:\quad \mathbb{R}^{d_c} \times \mathbb{R}^{p} \to \mathbb{R},
\end{equation}
which produces a scalar similarity between a child \(\bm{v}_c\) and a parent \(\bm{v}_p\). Let \( \{ \bm{v}_c \} = \{\bm{v}_c : c \in C_m\}\) and \( \{ \bm{v}_p \} = \{\bm{v}_p : p \in P_m\}\), the energy for \(E_m\) is defined as:
\begin{equation}
E_m\bigl(\{ \bm{v}_c \} , \{ \bm{v}_p \} \bigr)
\;=\;
-\sum_{c \in C_m}
\ln \Bigl(
\sum_{p \in P_m}
\exp\bigl(\mathrm{sim}(\bm{v}_c, \bm{v}_p)\bigr)
\Bigr).
\end{equation}
The global energy sums over all energy functions:
\begin{equation}
E\bigl(\{ \bm{v} \} \bigr)
\;=\;
\sum_{m=1}^M
E_m\bigl(\{ \bm{v}_c \} , \{ \bm{v}_p \} \bigr).
\end{equation}

\paragraph{Gradient Updates.}  
For a single node \(\bm{v}_a\), the gradient of the global energy \(E\) w.r.t.\ \(\bm{v}_a\) decomposes into two terms. Let \(\mathcal{M}_c(a) = \{m : a \in C_m\}\) denote the energy functions where \(\bm{v}_a\) acts as a \emph{child}, and \(\mathcal{M}_p(a) = \{m : a \in P_m\}\) the energy functions where \(\bm{v}_a\) acts as a \emph{parent}. Then:
\begin{equation}
\begin{aligned}
-\frac{\partial E}{\partial \bm{v}_a}
\;=\;&
\underbrace{
\sum_{m \in \mathcal{M}_c(a)} \sum_{p \in P_m}
\text{softmax}_{p} \Bigl(\mathrm{sim}(\bm{v}_a, \bm{v}_p)\Bigr)\,
\frac{\partial}{\partial \bm{v}_a}
\mathrm{sim}\bigl(\bm{v}_a, \bm{v}_p\bigr)
}_{\text{\(\bm{v}_a\) acting as a child}} \\
&+
\underbrace{
\sum_{m \in \mathcal{M}_p(a)} \sum_{c \in C_m}
\text{softmax}_{a} \Bigl(\mathrm{sim}(\bm{v}_c, \bm{v}_a)\Bigr)\,
\frac{\partial}{\partial \bm{v}_a}
\mathrm{sim}\bigl(\bm{v}_c, \bm{v}_a\bigr)
}_{\text{\(\bm{v}_a\) acting as a parent}}.
\end{aligned}
\end{equation}
The first term captures contributions from \(\bm{v}_a\) being explained by its parents, while the second term captures contributions from \(\bm{v}_a\) explaining its children.


\section{Gaussian Mixture Models}

\paragraph{Setup.} 
We have \(N\) data points (children) \(\bm{x}_i \in \mathbb{R}^d\), \(i \in C = \{1,\ldots,N\}\), and \(K\) mixture components (parents), each with mean \(\bm{\mu}_k \in \mathbb{R}^d\) and covariance \(\bm{\Sigma}_k\), \(k \in P = \{1,\ldots,K\}\).  Let \(\pi_k\) be the mixing proportion.

\paragraph{Similarity function.}
We define
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{\mu}_k\bigr)
\;=\;
\ln \pi_k
\;-\;
\tfrac12 \bigl(\bm{x}_i - \bm{\mu}_k\bigr)^\top
\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{GMM}}\!\Bigl(\{\bm{x}_i\}, \{\bm{\mu}_k\}\Bigr)
\;=\;
-\sum_{i=1}^N
\ln \Bigl(\sum_{k=1}^K
\exp\bigl(\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
If we differentiate w.r.t.\ \(\bm{\mu}_k\), then
\[
-\frac{\partial E^{\mathrm{GMM}}}{\partial \bm{\mu}_k}
\;=\;
\sum_{i=1}^N 
\text{softmax}_k\!\bigl(\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)\bigr)
\;\bm{\Sigma}_k^{-1}\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]
Setting this gradient to zero yields the usual GMM M-step:
\[
\bm{\mu}_k
\;=\;
\frac{\sum_{i=1}^N 
\text{softmax}_k\!\bigl(\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)\bigr)\;\bm{x}_i}
     {\sum_{i=1}^N
     \text{softmax}_k\!\bigl(\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)\bigr)}.
\]

\section{Hopfield Networks}

\paragraph{Setup.}
We have a set of \emph{children} data vectors \(\bm{x}_i \in \mathbb{R}^d\), \(i\in C=\{1,\ldots,N\}\), and a set of \emph{parent} memory vectors \(\bm{m}_\mu \in \mathbb{R}^d\), \(\mu\in P=\{1,\ldots,K\}\).

\paragraph{Similarity function.}
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{m}_\mu\bigr)
\;=\;
\bm{x}_i^\top \bm{m}_\mu.
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Hopfield}}\!\Bigl(\{\bm{x}_i\}, \{\bm{m}_\mu\}\Bigr)
\;=\;
-\sum_{i=1}^N
\ln \Bigl(\sum_{\mu=1}^K
\exp \bigl(\bm{x}_i^\top \bm{m}_\mu\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\begin{equation}
-\frac{\partial E^{\mathrm{Hopfield}}}{\partial \bm{x}_i}
\;=\;
\sum_{\mu=1}^K
\text{softmax}_{\mu}\!\bigl(\bm{x}_i^\top \bm{m}_\mu\bigr)\,
\bm{m}_\mu.
\end{equation}
\begin{equation}
-\frac{\partial E^{\mathrm{Hopfield}}}{\partial \bm{m}_\mu}
\;=\;
\sum_{i=1}^N
\text{softmax}_{\mu}\!\bigl(\bm{x}_i^\top \bm{m}_\mu\bigr)\,
\bm{x}_i.
\end{equation}

\section{Slot Attention}

\paragraph{Setup.}
Let \(\bm{x}_j \in \mathbb{R}^d\), \(j\in C=\{1,\ldots,N\}\) be the children (tokens), and \(\bm{\mu}_i \in \mathbb{R}^d\), \(i \in P=\{1,\ldots,S\}\) be the parents (slots).  
We typically apply linear transforms \(\bm{W}_K,\bm{W}_Q \in \mathbb{R}^{d \times d}\) to form
\[
\mathrm{sim}\bigl(\bm{x}_j,\bm{\mu}_i\bigr)
\;=\;
\bigl(\bm{W}_K\,\bm{x}_j\bigr)^\top
\bigl(\bm{W}_Q\,\bm{\mu}_i\bigr).
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Slot}}\!\Bigl(\{\bm{x}_j\}, \{\bm{\mu}_i\}\Bigr)
\;=\;
-\sum_{j=1}^N
\ln \Bigl(\sum_{i=1}^S
\exp \bigl(\mathrm{sim}(\bm{x}_j,\bm{\mu}_i)\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\begin{equation}
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{\mu}_i}
    \;=\;
    \sum_{j=1}^N
    \text{softmax}_{i}\!\Bigl(\mathrm{sim}(\bm{x}_j,\bm{\mu}_i)\Bigr)\,
    \bm{W}_Q^\top \bm{W}_K\,\bm{x}_j.
    \end{equation}
    
\begin{equation}
-\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{x}_j}
\;=\;
\sum_{i=1}^S
\text{softmax}_{i}\!\Bigl(\mathrm{sim}(\bm{x}_j,\bm{\mu}_i)\Bigr)\,
\bm{W}_K^\top \bm{W}_Q\,\bm{\mu}_i.
\end{equation}

\section{Self-Attention}

\paragraph{Setup.}
In self-attention, every node can act as both a child (query) and a parent (key). Concretely, let us have \(N\) tokens \(\{\bm{x}_1,\dots,\bm{x}_N\}\).  We form
\[
\bm{q}_i \;=\; \bm{W}^Q\,\bm{x}_i,
\quad
\bm{k}_i \;=\; \bm{W}^K\,\bm{x}_i,
\]
for \(i = 1,\ldots,N\).  
Thus the set \(C = \{1,\ldots,N\}\) and \(P = \{1,\ldots,N\}\) coincide, with
\[
\mathrm{sim}\bigl(\bm{x}_c,\bm{x}_p\bigr)
\;=\;
\bigl(\bm{W}^Q \bm{x}_c\bigr)^\top
\bigl(\bm{W}^K \bm{x}_p\bigr).
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{SA}}\!\Bigl(\{\bm{x}_i\}\Bigr)
\;=\;
-\sum_{c=1}^N
\ln \Bigl(\sum_{p=1}^N
\exp \Bigl(
(\bm{W}^Q \bm{x}_c)^\top
(\bm{W}^K \bm{x}_p)
\Bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
Since each \(\bm{x}_i\) is \emph{both} a child and a parent, its gradient is a sum of two terms (the child side and the parent side).  Writing it out explicitly:
\begin{equation}
    \begin{aligned}
    -\frac{\partial E^{\mathrm{SA}}}{\partial \bm{x}_i}
    \;=\;&
    \underbrace{
    \sum_{p=1}^N
    \text{softmax}_{p}\!\Bigl(
    (\bm{W}^Q \bm{x}_i)^\top
    (\bm{W}^K \bm{x}_p)
    \Bigr)\,
    \bm{W}_Q^\top \bm{W}_K \,\bm{x}_p
    }_{\text{child $i$ being explained by parents $p$}} \\
    &+
    \underbrace{
    \sum_{c=1}^N
    \text{softmax}_{i}\!\Bigl(
    (\bm{W}^Q \bm{x}_c)^\top
    (\bm{W}^K \bm{x}_i)
    \Bigr)\,
    \bm{W}_K^\top \bm{W}_Q \,\bm{x}_c
    }_{\text{parent $i$ explaining children $c$}}.
    \end{aligned}
    \end{equation}


\section{Spatiotemporal Attention}

\paragraph{Setup.}
We consider a hierarchy of \(L\) layers, each containing \(K_l\) latent variables of dimension \(D_l\). Concretely, let 
\(\bm{x}_{k,t}^{(l)} \in \mathbb{R}^{D_l}\)
denote the \(k\)-th variable (or ``slot'') in layer \(l\) at time \(t\). The lowest layer (\(l=1\)) has \(K_1 = N\) observed variables (e.g., pixels) and dimension \(D_1\) (e.g., \([x, y, r, g, b]\)), while higher layers have separate dimensions \(D_l\) and numbers of slots \(K_l\). Our goal is to define an energy that couples these variables \emph{vertically} (across layers), \emph{concurrently} (within the same layer and time), and \emph{temporally} (across time).

\paragraph{Similarity Functions.}
We introduce three types of similarity, each with its own projection matrices. For inter-layer connections (linking layers \(l-1\) and \(l\)), we define:
\begin{equation}
\mathrm{sim}_{v}^{(l)}(\bm{x}, \bm{x}')
=
\bigl(\bm{W}_{v}^{K,(l)}\,\bm{x}\bigr)^\top
\bigl(\bm{W}_{v}^{Q,(l)}\,\bm{x}'\bigr),
\end{equation}
For intra-layer (slots within the same layer and time):
\begin{equation}
\mathrm{sim}_{c}^{(l)}(\bm{x}, \bm{x}')
=
\bigl(\bm{W}_{c}^{Q,(l)}\,\bm{x}\bigr)^\top
\bigl(\bm{W}_{c}^{K,(l)}\,\bm{x}'\bigr),
\end{equation}
For temporal connections (the same slot across different times):
\begin{equation}
\mathrm{sim}_{t}^{(l)}(\bm{x}, \bm{x}')
=
\bigl(\bm{W}_{t}^{Q,(l)}\,\bm{x}\bigr)^\top
\bigl(\bm{W}_{t}^{K,(l)}\,\bm{x}'\bigr).
\end{equation}
Here, \(\bm{W}_{\bullet}^{Q,(l)}\) and \(\bm{W}_{\bullet}^{K,(l)}\) are learnable projection matrices for layer \(l\). The intra-layer and temporal parameters parallel the key-query mechanism in self-attention, while the inter-layer parameters are analogous to the inverted attention mechanism used in slot attention.


\paragraph{Energy}


At each layer \(l\), the total energy is split into three terms:
\begin{equation}
    \begin{aligned}
    E_{v}^{(l)}
    &=
    -\sum_{t=1}^T
    \sum_{c=1}^{K_{l-1}}
    \underbrace{
    \ln \Biggl(
    \sum_{k=1}^{K_l}
    \exp\bigl(\mathrm{sim}_{v}^{(l)}(\bm{x}_{c,t}^{(l-1)}, \bm{x}_{k,t}^{(l)})\bigr)
    \Biggr)
    }_{\text{Inter-layer energy}},
    \\
    E_{c}^{(l)}
    &=
    -\sum_{t=1}^T
    \sum_{k=1}^{K_l}
    \underbrace{
    \ln \Biggl(
    \sum_{\substack{k'=1 \\ k'\neq k}}^{K_l}
    \exp\bigl(\mathrm{sim}_{c}^{(l)}(\bm{x}_{k,t}^{(l)}, \bm{x}_{k',t}^{(l)})\bigr)
    \Biggr)
    }_{\text{Intra-layer energy}},
    \\
    E_{t}^{(l)}
    &=
    -\sum_{k=1}^{K_l}
    \sum_{t=2}^T
    \underbrace{
    \ln \Biggl(
    \sum_{t'<t}
    \exp\bigl(\mathrm{sim}_{t}^{(l)}(\bm{x}_{k,t}^{(l)}, \bm{x}_{k,t'}^{(l)})\bigr)
    \Biggr)
    }_{\text{Temporal energy}}.
    \end{aligned}
    \end{equation}
Summing these over all layers \(l \in \{1, \dots, L\}\) defines the total energy \(E\). These terms correspond to inter-layer connections (\(E_{v}^{(l)}\)) and match the energy function for slot attention, intra-layer connections (\(E_{c}^{(l)}\)) which match the energy function for self attention, and temporal connections (\(E_{t}^{(l)}\)), which match the energy function for \emph{causal} self attention (as each variable is only explained by past variables).

\paragraph{Gradients}
Consider a single variable \(\bm{x}_{k,t}^{(l)}\). Its gradient with respect to the energy decomposes into four parts:

\begin{equation}
-\frac{\partial E}{\partial \bm{x}_{k,t}^{(l)}} = 
\underbrace{-\frac{\partial E_{v}^{(l)}}{\partial \bm{x}_{k,t}^{(l)}}}_{\text{bottom-up}} 
+ 
\underbrace{-\frac{\partial E_{v}^{(l+1)}}{\partial \bm{x}_{k,t}^{(l)}}}_{\text{top-down}} 
+ 
\underbrace{-\frac{\partial E_{c}^{(l)}}{\partial \bm{x}_{k,t}^{(l)}}}_{\text{intra-layer}} 
+ 
\underbrace{-\frac{\partial E_{t}^{(l)}}{\partial \bm{x}_{k,t}^{(l)}}}_{\text{temporal}}.
\end{equation}

\paragraph{Bottom-up:}

We define a similarity matrix \(\bm{A}_{c,k}\), representing the interactions between \(\bm{x}_{k,t}^{(l)}\) in layer \(l\) and \(\bm{x}_{c,t}^{(l-1)}\) in the layer below.
\begin{equation}
\bm{A}_{c,k} = \mathrm{sim}_{v}^{(l)}(\bm{x}_{c,t}^{(l-1)}, \bm{x}_{k,t}^{(l)}),
\end{equation}

The gradient with respect to \(\bm{x}_{k,t}^{(l)}\) is:
\begin{equation}
\begin{aligned}
-\frac{\partial E_{v}^{(l)}}{\partial \bm{x}_{k,t}^{(l)}}
&=
\sum_{c=1}^{K_{l-1}}
\mathrm{softmax}_{k}\bigl(\bm{A}_{c,k}\bigr)
\bm{W}_{v}^{Q,(l)\top} \bm{W}_{v}^{K,(l)} \bm{x}_{c,t}^{(l-1)}.
\end{aligned}
\end{equation}

This term aggregates contributions from the children in layer \(l-1\), weighted by the attention \(\mathrm{softmax}_{k}(\bm{A}_{c,k})\), and is analogous to Slot Attention or Gaussian mixture models.

\paragraph{Top-down:}

We define a similarity matrix \(\bm{A}_{k,p}\), representing the interactions between \(\bm{x}_{k,t}^{(l)}\) in layer \(l\) and \(\bm{x}_{p,t}^{(l+1)}\) in the layer above.
\begin{equation}
\bm{A}_{k,p} = \mathrm{sim}_{v}^{(l+1)}(\bm{x}_{k,t}^{(l)}, \bm{x}_{p,t}^{(l+1)}),
\end{equation}

The gradient with respect to \(\bm{x}_{k,t}^{(l)}\) is:
\begin{equation}
\begin{aligned}
-\frac{\partial E_{v}^{(l+1)}}{\partial \bm{x}_{k,t}^{(l)}}
&=
\sum_{p=1}^{K_{l+1}}
\mathrm{softmax}_{p}\bigl(\bm{A}_{k,p}\bigr)
\bm{W}_{v}^{K,(l+1)\top} \bm{W}_{v}^{Q,(l+1)} \bm{x}_{p,t}^{(l+1)}.
\end{aligned}
\end{equation}
This term captures the influence of \(\bm{x}_{k,t}^{(l)}\) being treated as a child, weighted by the attention \(\mathrm{softmax}_{p}(\bm{A}_{k,p})\), and reflects the top-down influence from layer \(l+1\), and is analogous to Hopfield attention or the parent term in self-attention.


\paragraph{Intra-layer:}

We define two similarity matrices, \(\bm{A}_{k,k'}\) and \(\bm{A}_{k',k}\), representing the bidirectional interactions between \(\bm{x}_{k,t}^{(l)}\) and other slots \(\bm{x}_{k',t}^{(l)}\):
\begin{equation}
\begin{aligned}
\bm{A}_{k,k'} &= \mathrm{sim}_{c}^{(l)}(\bm{x}_{k,t}^{(l)}, \bm{x}_{k',t}^{(l)}), \\
\bm{A}_{k',k} &= \mathrm{sim}_{c}^{(l)}(\bm{x}_{k',t}^{(l)}, \bm{x}_{k,t}^{(l)}), \\
\end{aligned}
\end{equation}
where 
\[
\mathrm{sim}_{c}^{(l)}(\bm{x}_{k,t}^{(l)}, \bm{x}_{k',t}^{(l)}) 
= \bigl(\bm{W}_{c}^{Q,(l)} \bm{x}_{k,t}^{(l)}\bigr)^\top \bigl(\bm{W}_{c}^{K,(l)} \bm{x}_{k',t}^{(l)}\bigr).
\]

The gradient with respect to \(\bm{x}_{k,t}^{(l)}\) is:
\begin{equation}
\begin{aligned}
-\frac{\partial E_{c}^{(l)}}{\partial \bm{x}_{k,t}^{(l)}}
&=
\underbrace{
\sum_{k'\neq k}
\mathrm{softmax}_{k}\bigl(\bm{A}_{k',k}\bigr)
\bm{W}_{c}^{K,(l)\top} \bm{W}_{c}^{Q,(l)} \bm{x}_{k',t}^{(l)}
}_{\text{\(\bm{x}_{k,t}^{(l)}\) acting as parent (explaining others)}} 
\\
&\quad +
\underbrace{
\sum_{k'\neq k}
\mathrm{softmax}_{k'}\bigl(\bm{A}_{k, k'}\bigr)
\bm{W}_{c}^{Q,(l)\top} \bm{W}_{c}^{K,(l)} \bm{x}_{k',t}^{(l)}
}_{\text{\(\bm{x}_{k,t}^{(l)}\) acting as child (being explained by others)}}.
\end{aligned}
\end{equation}
The first term captures the influence of \(\bm{x}_{k,t}^{(l)}\) as a parent, aggregating the contributions from other slots \(\bm{x}_{k',t}^{(l)}\) it explains, weighted by \(\mathrm{softmax}_{k'}(\bm{A}_{k,k'})\). The second term reflects \(\bm{x}_{k,t}^{(l)}\)'s role as a child, being explained by other slots \(\bm{x}_{k',t}^{(l)}\), weighted by \(\mathrm{softmax}_{k}(\bm{A}_{k',k})\).


\paragraph{Temporal:}

\paragraph{Setup.}
We define a similarity matrix \(\bm{A}_{t,t'}\) representing the interaction between a slot \(\bm{x}_{k,t}^{(l)}\) at time \(t\) and its past representation \(\bm{x}_{k,t'}^{(l)}\) at time \(t'\), for \(t'<t\):
\begin{equation}
\bm{A}_{t,t'} \;=\;
\mathrm{sim}_{t}^{(l)}\!\Bigl(\bm{x}_{k,t}^{(l)},\,\bm{x}_{k,t'}^{(l)}\Bigr)
\;=\;
\bigl(\bm{W}_{t}^{Q,(l)}\,\bm{x}_{k,t}^{(l)}\bigr)^\top
\bigl(\bm{W}_{t}^{K,(l)}\,\bm{x}_{k,t'}^{(l)}\bigr).
\end{equation}

\paragraph{Gradient.}
Assuming causal attention (i.e.\ no future times), the gradient w.r.t.\ \(\bm{x}_{k,t}^{(l)}\) is:
\begin{equation}
\begin{aligned}
-\frac{\partial E_{t}^{(l)}}{\partial \bm{x}_{k,t}^{(l)}}
&=
\sum_{t'<t}
\mathrm{softmax}_{t'}\bigl(\bm{A}_{t,t'}\bigr)\,
\bm{W}_{t}^{Q,(l)\top}\,\bm{W}_{t}^{K,(l)}\,\bm{x}_{k,t'}^{(l)}.
\end{aligned}
\end{equation}
Thus \(\bm{x}_{k,t}^{(l)}\) is influenced only by its own past \(\bm{x}_{k,t'}^{(l)}\) (\(t'<t\)), as in causal self-attention.



\paragraph{Remark (Full Version).}
We can generalize to let each latent \(\bm{s}_{k,t}\) attend \emph{all} data features and \emph{all} slot latents at \emph{any} time.  This merges the three energies into a single large log-sum-exp, imposing direct competition among data, concurrency, and temporal parents, but increases complexity to \(\mathcal{O}((N + K)\,T)^2\). 


\section{Dynamic Mixture Attention}

\paragraph{Setup.}
We consider a time series of observations \(\{\bm{x}_t\}_{t=1}^T\), where each \(\bm{x}_t \in \mathbb{R}^d\). The transition from \(\bm{x}_t\) to \(\bm{x}_{t+1}\) is governed by \(K\) distinct dynamical modes, each parameterized by \(\{\bm{A}_k, \bm{b}_k\}\) for \(k \in \{1, \dots, K\}\). Additionally, each \(\bm{x}_t\) can attend to \(K\) \emph{parents} \(\{\bm{z}_k\}_{k=1}^K\), which may represent previous states \(\bm{x}_{t'}\) for \(t' < t\), concurrent slots at the same time step, or variables from other layers. This setup generalizes various attention mechanisms, including causal self-attention and inter-layer attention.

\paragraph{Similarity Functions.}
We define two similarity measures for each mode \(k\). The first captures the attention-based similarity between the current state \(\bm{x}_t\) and its parent \(\bm{z}_k\) using learnable projection matrices \(\bm{W}^Q\) and \(\bm{W}^K\):
\[
\mathrm{sim}_{\mathrm{parents}}\bigl(\bm{x}_t, \bm{z}_k\bigr)
\;=\;
\bigl(\bm{W}^Q\,\bm{x}_t\bigr)^\top\,\bigl(\bm{W}^K\,\bm{z}_k\bigr).
\]
The second similarity term models the dynamical compatibility between \(\bm{x}_t\) and \(\bm{x}_{t+1}\) under mode \(k\):
\[
\mathrm{sim}_{\mathrm{dynamics}}\bigl(\bm{x}_t, \bm{x}_{t+1}; \bm{A}_k, \bm{b}_k\bigr)
\;=\;
-\tfrac{1}{2}\,\bigl\|\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr\|^2.
\]

\paragraph{Energy.}
Each transition \(\bm{x}_t \to \bm{x}_{t+1}\) is associated with \(K\) modes, each combining attention to a parent and a dynamical transformation. The energy is defined as:
\begin{equation}
E^{\mathrm{DMA}}\!\Bigl(\{\bm{x}_t\}, \{\bm{A}_k,\bm{b}_k\}, \{\bm{z}_k\}\Bigr)
\;=\;
-\sum_{t=1}^{T-1}
\ln \Biggl(
\sum_{k=1}^K
\exp\biggl(
\mathrm{sim}_{\mathrm{parents}}\bigl(\bm{x}_t, \bm{z}_k\bigr)
\;+\;
\mathrm{sim}_{\mathrm{dynamics}}\bigl(\bm{x}_t, \bm{x}_{t+1}; \bm{A}_k, \bm{b}_k\bigr)
\biggr)
\Biggr).
\end{equation}
This formulation allows each mode \(k\) to simultaneously attend to a parent \(\bm{z}_k\) and explain the transition through \(\bm{A}_k\) and \(\bm{b}_k\).

\paragraph{Gradients.}
Define the attention weights as:
\[
\alpha_{t,k}
\;=\;
\text{softmax}_k\Bigl(
\mathrm{sim}_{\mathrm{parents}}\bigl(\bm{x}_t, \bm{z}_k\bigr)
\;+\;
\mathrm{sim}_{\mathrm{dynamics}}\bigl(\bm{x}_t, \bm{x}_{t+1}; \bm{A}_k, \bm{b}_k\bigr)
\Bigr).
\]
Then,
\[
-\frac{\partial E^{\mathrm{DMA}}}{\partial \bm{x}_{t+1}}
=
\sum_{k=1}^K
\alpha_{t,k}
\Biggl[
\underbrace{
\frac{\partial}{\partial \bm{x}_{t+1}}
\mathrm{sim}_{\mathrm{parents}}(\bm{x}_t, \bm{z}_k)
}_{\;=\,0}
\;+\;
\underbrace{
\frac{\partial}{\partial \bm{x}_{t+1}}
\mathrm{sim}_{\mathrm{dynamics}}(\bm{x}_t, \bm{x}_{t+1}; \bm{A}_k, \bm{b}_k)
}_{\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k}
\Biggr]
\]
\[
=
\sum_{k=1}^K
\alpha_{t,k}
\bigl[\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr].
\]

\[
-\frac{\partial E^{\mathrm{DMA}}}{\partial \bm{x}_t}
=
\sum_{k=1}^K
\alpha_{t,k}
\Biggl[
\underbrace{
\frac{\partial}{\partial \bm{x}_t}
\mathrm{sim}_{\mathrm{parents}}(\bm{x}_t, \bm{z}_k)
}_{\bm{W}^Q{}^\top\,\bm{W}^K\,\bm{z}_k}
\;+\;
\underbrace{
\frac{\partial}{\partial \bm{x}_t}
\mathrm{sim}_{\mathrm{dynamics}}(\bm{x}_t, \bm{x}_{t+1}; \bm{A}_k, \bm{b}_k)
}_{-\,\bm{A}_k^\top\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)}
\Biggr]
\]
\[
=
\sum_{k=1}^K
\alpha_{t,k}
\Bigl[
\bm{W}^Q{}^\top\,\bm{W}^K\,\bm{z}_k
\;-\;
\bm{A}_k^\top\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)
\Bigr].
\]

Additionally, gradients with respect to the parameters \(\bm{A}_k\) and \(\bm{b}_k\) are given by:
\[
-\frac{\partial E^{\mathrm{DMA}}}{\partial \bm{A}_k}
=
\sum_{t=1}^{T-1}
\alpha_{t,k}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)\,\bm{x}_t^\top,
\]
\[
-\frac{\partial E^{\mathrm{DMA}}}{\partial \bm{b}_k}
=
\sum_{t=1}^{T-1}
\alpha_{t,k}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]

\paragraph{Fixed Point for \(\bm{x}_{t+1}\).}
Setting the gradient with respect to \(\bm{x}_{t+1}\) to zero yields the fixed point equation:
\[
\bm{x}_{t+1}
\;=\;
\sum_{k=1}^K
\alpha_{t,k}\,
\bigl(\bm{A}_k\,\bm{x}_t + \bm{b}_k\bigr),
\]
where \(\alpha_{t,k}\) itself depends on \(\bm{x}_{t+1}\). This equation must be solved iteratively, typically via fixed-point iteration or gradient-based optimization methods to achieve consistency. 


\section{Predictive coding}

\paragraph{Setup.}
We again have child vectors \(\{\bm{x}_i\}_{i=1}^N\) and a set of parent \(\bm{\mu}_k \in \mathbb{R}^d, k=1,\dots,K\).  However, rather than a direct difference \(\bm{x}_i-\bm{\mu}_k\), let us assume the \emph{model} maps \(\bm{\mu}_k\) through some non-linear function \(f_\phi(\cdot)\) before comparing to \(\bm{x}_i\).  For instance, \(f_\phi\) could be a neural network.

\paragraph{Similarity function.}
Define
\[
\mathrm{sim}\bigl(\bm{x}_i,\bm{\mu}_k\bigr)
\;=\;
-\,\tfrac12\,
\bigl\|\bm{x}_i \;-\; f_\phi(\bm{\mu}_k)\bigr\|^2.
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{PC}}\!\Bigl(\{\bm{x}_i\}, \{\bm{\mu}_k\}\Bigr)
\;=\;
-\sum_{i=1}^N
\ln \Bigl(\sum_{k=1}^K
\exp\Bigl(-\tfrac12\,\bigl\|\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr\|^2\Bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\[
\alpha_{i,k}
\;=\;
\mathrm{softmax}_k\Bigl(
-\,\tfrac12\,\bigl\|\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr\|^2
\Bigr).
\]

\[
-\frac{\partial E^{\mathrm{PC}}}{\partial \bm{x}_i}
\;=\;
\sum_{k=1}^K 
\alpha_{i,k}\,\bigl(\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr).
\]
\[
-\frac{\partial E^{\mathrm{PC}}}{\partial \bm{\mu}_k}
\;=\;
\sum_{i=1}^N 
\alpha_{i,k}\,
\underbrace{
\frac{\partial f_\phi(\bm{\mu}_k)}{\partial \bm{\mu}_k}
}_{\text{Jacobian of }f_\phi}
\;\bigl(\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr).
\]
Here, \(\tfrac{\partial f_\phi(\bm{\mu}_k)}{\partial \bm{\mu}_k}\) is the \(d\times d\) Jacobian (or more general shape if \(\bm{\mu}_k\) and \(f_\phi(\bm{\mu}_k)\) differ in dimension). 

 
\section{Cross Attention}

\paragraph{Setup.}
We have a set of child vectors (queries) \(\bm{Q}\in\mathbb{R}^{d\times N_Q}\) and a set of parent vectors (keys) \(\bm{K}\in\mathbb{R}^{d\times N_K}\).  Let
\[
C \;=\;\{1,\ldots,N_Q\}, 
\quad
P \;=\;\{1,\ldots,N_K\},
\]
so \(\bm{v}_c = \bm{q}_c\) is the \(c\)-th query, and \(\bm{v}_p = \bm{k}_p\) is the \(p\)-th key.  Suppose we have learnable weight matrices \(\bm{W}^Q,\bm{W}^K \in \mathbb{R}^{d \times d}\).  Then
\[
\bm{q}_c 
\;=\;
\bm{W}^Q\bm{x}^Q_c,
\quad
\bm{k}_p 
\;=\;
\bm{W}^K\bm{x}^K_p,
\]
where \(\bm{x}^Q_c\) is the raw \(c\)-th query token and \(\bm{x}^K_p\) the raw \(p\)-th key token.

\paragraph{Similarity function.}
\[
\mathrm{sim}\bigl(\bm{q}_c, \bm{k}_p\bigr)
\;=\;
\bm{q}_c^\top \bm{k}_p.
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Cross}}\!\Bigl(\{\bm{q}_c\}, \{\bm{k}_p\}\Bigr)
\;=\;
-\sum_{c=1}^{N_Q}
\ln \Bigl(\sum_{p=1}^{N_K}
\exp\bigl(\bm{q}_c^\top\,\bm{k}_p\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\begin{equation}
-\frac{\partial E^{\mathrm{Cross}}}{\partial \bm{q}_c}
\;=\;
\sum_{p=1}^{N_K}
\text{softmax}_{p}\!\Bigl(\bm{q}_c^\top \bm{k}_p \Bigr)\,
\bm{k}_p.
\end{equation}
\begin{equation}
-\frac{\partial E^{\mathrm{Cross}}}{\partial \bm{k}_p}
\;=\;
\sum_{c=1}^{N_Q}
\text{softmax}_{p}\!\Bigl(\bm{q}_c^\top \bm{k}_p \Bigr)\,
\bm{q}_c.
\end{equation}
When mapping back to the raw tokens \(\bm{x}^Q_c\) or \(\bm{x}^K_p\), chain-rule multiplies by \(\bm{W}^Q\) or \(\bm{W}^K\), respectively.


\section{Switching Linear Dynamical System}

\paragraph{Setup.}
Consider a time series of observations \(\{\bm{x}_t\}_{t=1}^T\), where each \(\bm{x}_t \in \mathbb{R}^d\).  
We assume there are \(K\) distinct (linear) dynamical modes, each with parameters \(\{\bm{A}_k, \bm{b}_k, \bm{\Sigma}_k\}\). Let \(\pi_k\) be the mixing weight of mode \(k\).  A typical \emph{switching linear dynamical system} (SLDS) posits:
\[
\bm{x}_{t+1}
\;\approx\;
\bm{A}_k\,\bm{x}_t \;+\; \bm{b}_k,
\quad
k \in \{1,\dots,K\},
\]
with Gaussian noise \(\bm{\Sigma}_k\).  
We treat \(\bm{x}_{t+1}\) as a \emph{child} and the \(\{\bm{A}_k, \bm{b}_k\}\) (together with \(\bm{x}_t\)) as \emph{parents} in a mixture-of-linear-dynamics fashion.

\paragraph{Similarity function.}
Define, for each mode \(k\),
\[
\mathrm{sim}\bigl(\bm{x}_{t+1}, \bm{x}_t; \bm{A}_k, \bm{b}_k\bigr)
\;=\;
\ln \pi_k
\;-\;
\tfrac12\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)^\top
\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]

\paragraph{Energy.}
Summing over all time steps \(t = 1,\ldots,T-1\), we write
\begin{equation}
E^{\mathrm{SLDS}}\!\Bigl(\{\bm{x}_t\}, \{\bm{A}_k, \bm{b}_k\}\Bigr)
\;=\;
-\sum_{t=1}^{T-1}\,
\ln \Bigl(\sum_{k=1}^K
\exp\!\Bigl(
\mathrm{sim}\bigl(\bm{x}_{t+1}, \bm{x}_t; \bm{A}_k, \bm{b}_k\bigr)
\Bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
Let 
\[
\alpha_{t,k} 
\;=\;
\mathrm{softmax}_{k}\Bigl(
\mathrm{sim}\bigl(\bm{x}_{t+1}, \bm{x}_t; \bm{A}_k, \bm{b}_k\bigr)\Bigr),
\]
i.e.\ the normalized exponent for mode \(k\).  

\[
-\frac{\partial E^{\mathrm{SLDS}}}{\partial \bm{x}_{t+1}}
\;=\;
\sum_{k=1}^K 
\alpha_{t,k}\,\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]
\[
-\frac{\partial E^{\mathrm{SLDS}}}{\partial \bm{x}_t}
\;=\;
\sum_{k=1}^K 
\alpha_{t,k}\,
\bigl(-\,\bm{A}_k^\top\,\bm{\Sigma}_k^{-1}\bigr)\,
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)
\quad (t=1,\dots,T-1).
\]
\[
-\frac{\partial E^{\mathrm{SLDS}}}{\partial \bm{A}_k}
\;=\;
\sum_{t=1}^{T-1}
\alpha_{t,k}\,\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)\,\bm{x}_t^\top.
\]
\[
-\frac{\partial E^{\mathrm{SLDS}}}{\partial \bm{b}_k}
\;=\;
\sum_{t=1}^{T-1}
\alpha_{t,k}\,\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]

\section{Layer Normalization}

\paragraph{Setup.}
We consider a batch of input vectors \(\{\bm{x}_i\}_{i=1}^B\), where each vector \(\bm{x}_i \in \mathbb{R}^D\). Each vector is normalized by subtracting the mean and dividing by the standard deviation, with learnable scaling and bias parameters \(\gamma, \delta \in \mathbb{R}^D\). For numerical stability, a small constant \(\epsilon > 0\) is added to the variance.

\paragraph{Energy.}
The energy for layer normalization is given by:
\[
E^{\mathrm{LN}}(\{\bm{x}_i\}) = \sum_{i=1}^B \left[ \gamma \sqrt{\frac{1}{D} \sum_{j=1}^D (x_{ij} - \bar{\bm{x}}_i)^2 + \epsilon} + \sum_{j=1}^D \delta_j x_{ij} \right],
\]
where:
\[
\bar{\bm{x}}_i = \frac{1}{D} \sum_{j=1}^D x_{ij}.
\]

\paragraph{Derivative.}
The normalized outputs are obtained as the derivative of the energy with respect to the inputs:
\[
\frac{\partial E^{\mathrm{LN}}}{\partial x_{ij}} = \gamma_j \frac{x_{ij} - \bar{\bm{x}}_i}{\sqrt{\frac{1}{D} \sum_{k=1}^D (x_{ik} - \bar{\bm{x}}_i)^2 + \epsilon}} + \delta_j.
\]
\[
\bar{\bm{x}}_i = \frac{1}{D} \sum_{k=1}^D x_{ik}.
\]


\section{Coordinate ascent}
\label{sec:generic-slot-attention-em}
    
    Consider an energy of the form
    \[
    E(\{\bm{x}_j\}, \{\bm{\mu}_i\}; \theta)
    \;=\;
    -\sum_{j=1}^N
    \ln\Bigl(\sum_{i=1}^S
    \exp\bigl(\mathrm{sim}_\theta(\bm{x}_j,\bm{\mu}_i)\bigr)\Bigr),
    \]
    with \(\theta\) denoting the parameters of the similarity function. In an EM procedure, we alternate:
    \begin{itemize}
        \item \textbf{E-step}: Update latent variables using fixed \(\theta\).
        \item \textbf{M-step}: Update parameters \(\theta\) in closed form given fixed latent assignments.
    \end{itemize}
    
    For \emph{slot attention}, we have:
    \[
    \theta = \{\bm{W}_K,\bm{W}_Q\},
    \] 
    \[
    \mathrm{sim}_\theta(\bm{x}_j,\bm{\mu}_i)
    \;=\;
    (\bm{W}_K\bm{x}_j)^\top(\bm{W}_Q\bm{\mu}_i).
    \]
    \[
    \mathbf{A} 
    \quad\text{with entries}\quad 
    A_{ji}
    \;=\;
    \mathrm{softmax}_i\Bigl(
    (\bm{W}_K\bm{x}_j)^\top(\bm{W}_Q\bm{\mu}_i)
    \Bigr).
    \]
    
    \subsection*{E-step: Update Slots}
    Fix \(\bm{W}_K,\bm{W}_Q\). The gradient for each slot \(\bm{\mu}_i\) is
    \[
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{\mu}_i}
    =
    \sum_{j=1}^N
    A_{ji}\,
    \bm{W}_Q^\top\bm{W}_K\bm{x}_j.
    \]
    Use this gradient to iteratively update \(\{\bm{\mu}_i\}\) until convergence.
    
    \subsection*{M-step: Update Parameters}
    
    Given fixed slots \(\{\bm{\mu}_i\}\) and attention matrix \(\mathbf{A}\), we aim to update the parameters \(\bm{W}_K,\bm{W}_Q\). This is motivated by setting the gradient of the energy with respect to these parameters to zero:
    \[
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{W}_K} = 0,
    \qquad
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{W}_Q} = 0.
    \]
    Under the fixed assignments provided by \(\mathbf{A}\) and slots \(\{\bm{\mu}_i\}\), these conditions are equivalent to solving a weighted least-squares problem. Specifically, we consider minimizing the objective
    \[
    \min_{\bm{W}_K,\bm{W}_Q}
    \sum_{j=1}^N\sum_{i=1}^S
    A_{ji}\,
    \Bigl\|\bm{W}_K\bm{x}_j - \bm{W}_Q\bm{\mu}_i\Bigr\|^2,
    \]
    since the stationary point of this quadratic form corresponds to zero gradients with respect to \(\bm{W}_K,\bm{W}_Q\).


    \paragraph{Update \(\bm{W}_Q\):}
    Differentiate w.r.t.\ \(\bm{W}_Q\), set to zero:
    \[
    \sum_{j,i}A_{ji}\,
    \Bigl(\bm{W}_K\bm{x}_j - \bm{W}_Q\bm{\mu}_i\Bigr)
    \bm{\mu}_i^\top
    \;=\;0,
    \]
    yielding
    \[
    \bm{W}_Q
    \;=\;
    \Biggl(
    \sum_{j,i}A_{ji}\,
    \bm{W}_K\bm{x}_j\,\bm{\mu}_i^\top
    \Biggr)
    \Biggl(\sum_{j,i}A_{ji}\,
    \bm{\mu}_i\bm{\mu}_i^\top
    \Biggr)^{-1}.
    \]
    \paragraph{Update \(\bm{W}_K\):}
    Similarly, differentiate w.r.t.\ \(\bm{W}_K\), set to zero:
    \[
    \sum_{j,i}A_{ji}\,
    \Bigl(\bm{W}_K\bm{x}_j - \bm{W}_Q\bm{\mu}_i\Bigr)
    \bm{x}_j^\top
    \;=\;0,
    \]
    yielding
    \[
    \bm{W}_K
    \;=\;
    \Biggl(
    \sum_{j,i}A_{ji}\,
    \bm{W}_Q\bm{\mu}_i\,\bm{x}_j^\top
    \Biggr)
    \Biggl(\sum_{j,i}A_{ji}\,
    \bm{x}_j\bm{x}_j^\top
    \Biggr)^{-1}.
    \]
    
    \paragraph{Iterate:}
    Alternate between the E-step (updating \(\{\bm{\mu}_i\}\)) and the M-step (updating \(\bm{W}_K,\bm{W}_Q\)) until convergence.


  
    
    
\clearpage
\appendix

\section{Appendix: Derivation of Gradient Updates}

Let us consider a generic term:
\[
-\ln \Bigl(\,\sum_{m=1}^M \exp \bigl(f_m(\bm{x})\bigr)\Bigr),
\]
where \(\bm{x}\in \mathbb{R}^d\) is some variable, and each \(f_m\) is a scalar function.
We compute its gradient:
\begin{align*}
\frac{\partial}{\partial \bm{x}}
\biggl[
-\ln \Bigl(\,\sum_{m=1}^M \exp \bigl(f_m(\bm{x})\bigr)\Bigr)
\biggr]
&=\;
-\frac{1}{\sum_{m'} \exp \bigl(f_{m'}(\bm{x})\bigr)}
\;\sum_{m=1}^M
\exp \bigl(f_m(\bm{x})\bigr)\;\frac{\partial f_m(\bm{x})}{\partial \bm{x}}
\\[6pt]
&=\;
-\sum_{m=1}^M
\biggl[
\frac{\exp\!\bigl(f_m(\bm{x})\bigr)}{\sum_{m'} \exp \bigl(f_{m'}(\bm{x})\bigr)}
\biggr]
\;\frac{\partial f_m(\bm{x})}{\partial \bm{x}}.
\end{align*}
Defining
\(\mathrm{softmax}_m\bigl(f(\bm{x})\bigr) 
= 
\tfrac{\exp(f_m(\bm{x}))}{\sum_{m'} \exp(f_{m'}(\bm{x}))},\)
this is
\[
-\sum_{m=1}^M
\mathrm{softmax}_m \bigl(f(\bm{x})\bigr)
\;\frac{\partial f_m(\bm{x})}{\partial \bm{x}},
\]
which matches the \(\mathrm{softmax}\)-weighted gradient structure.


\section{Appendix}
\label{sec:deriv-sim-models}

Here, we collect the explicit partial derivatives of \(\mathrm{sim}\) for each model discussed.

\subsection*{Gaussian Mixture Models}
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{\mu}_k\bigr)
\;=\;
\ln \pi_k
\;-\;
\tfrac12 \bigl(\bm{x}_i - \bm{\mu}_k\bigr)^\top
\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]
\[
\frac{\partial}{\partial \bm{\mu}_k} \mathrm{sim}(\bm{x}_i,\bm{\mu}_k)
\;=\;
\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]

\subsection*{Cross Attention}
\[
\mathrm{sim}\bigl(\bm{q}_c, \bm{k}_p\bigr)
\;=\;
\bm{q}_c^\top \bm{k}_p.
\]
\[
\frac{\partial}{\partial \bm{q}_c} \mathrm{sim}(\bm{q}_c,\bm{k}_p)
\;=\;
\bm{k}_p,
\quad
\frac{\partial}{\partial \bm{k}_p} \mathrm{sim}(\bm{q}_c,\bm{k}_p)
\;=\;
\bm{q}_c.
\]

\subsection*{Hopfield Networks}
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{m}_\mu\bigr)
\;=\;
\bm{x}_i^\top \bm{m}_\mu.
\]
\[
\frac{\partial}{\partial \bm{x}_i} \mathrm{sim}(\bm{x}_i,\bm{m}_\mu)
\;=\;
\bm{m}_\mu,
\quad
\frac{\partial}{\partial \bm{m}_\mu} \mathrm{sim}(\bm{x}_i,\bm{m}_\mu)
\;=\;
\bm{x}_i.
\]

\subsection*{Slot Attention}
\[
\mathrm{sim}\bigl(\bm{x}_j,\bm{\mu}_i\bigr)
\;=\;
(\bm{W}_K\,\bm{x}_j)^\top
(\bm{W}_Q\,\bm{\mu}_i).
\]

\[
\frac{\partial}{\partial \bm{x}_j} \mathrm{sim}(\bm{x}_j,\bm{\mu}_i)
\;=\;
\bm{W}_K^\top \,\bm{W}_Q\,\bm{\mu}_i,
\quad
\frac{\partial}{\partial \bm{\mu}_i} \mathrm{sim}(\bm{x}_j,\bm{\mu}_i)
\;=\;
\bm{W}_Q^\top \,\bm{W}_K\,\bm{x}_j.
\]

\subsection*{Self-Attention}
\[
\mathrm{sim}\bigl(\bm{x}_c,\bm{x}_p\bigr)
\;=\;
(\bm{W}^Q \bm{x}_c)^\top\,(\bm{W}^K \bm{x}_p).
\]
\[
\frac{\partial}{\partial \bm{x}_c} \mathrm{sim}(\bm{x}_c,\bm{x}_p)
\;=\;
{\bm{W}^Q}^\top\,\bm{W}^K\,\bm{x}_p,
\quad
\frac{\partial}{\partial \bm{x}_p} \mathrm{sim}(\bm{x}_c,\bm{x}_p)
\;=\;
{\bm{W}^K}^\top\,\bm{W}^Q\,\bm{x}_c.
\]

\subsection*{Switching Linear Dynamical System}
\[
\mathrm{sim}\bigl(\bm{x}_{t+1}, \bm{x}_t; \bm{A}_k, \bm{b}_k\bigr)
\;=\;
\ln \pi_k
\;-\;
\tfrac12\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)^\top
\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]
\[
\frac{\partial}{\partial \bm{x}_{t+1}}\mathrm{sim}(\bm{x}_{t+1},\bm{x}_t;\bm{A}_k,\bm{b}_k)
\;=\;
\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr),
\]
\[
\frac{\partial}{\partial \bm{x}_t}\mathrm{sim}(\bm{x}_{t+1},\bm{x}_t;\bm{A}_k,\bm{b}_k)
\;=\;
-\,\bm{A}_k^\top \,\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr),
\]
\[
\frac{\partial}{\partial \bm{A}_k}\mathrm{sim}(\bm{x}_{t+1},\bm{x}_t;\bm{A}_k,\bm{b}_k)
\;=\;
\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)\,\bm{x}_t^\top,
\]
\[
\frac{\partial}{\partial \bm{b}_k}\mathrm{sim}(\bm{x}_{t+1},\bm{x}_t;\bm{A}_k,\bm{b}_k)
\;=\;
\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]

\subsection*{Predictive Coding}
\[
\mathrm{sim}\bigl(\bm{x}_i,\bm{\mu}_k\bigr)
\;=\;
-\,\tfrac12\,
\bigl\|\bm{x}_i \;-\; f_\phi(\bm{\mu}_k)\bigr\|^2.
\]
\[
\frac{\partial}{\partial \bm{x}_i}\,\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)
\;=\;
\bm{x}_i - f_\phi(\bm{\mu}_k),
\]
\[
\frac{\partial}{\partial \bm{\mu}_k}\,\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)
\;=\;
\frac{\partial f_\phi(\bm{\mu}_k)}{\partial \bm{\mu}_k}
\;\bigl(\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr).
\]


\end{document}




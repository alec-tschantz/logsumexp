\documentclass{article}
\usepackage{amsmath,amssymb,bm}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\begin{document}

\title{Attention via \(\log\sum\exp\) energy}
\author{Alexander Tschantz}
\date{\today}
\maketitle

\section{Gaussian Mixture Models (GMMs)}

\paragraph{Setup.}
Let \(\bm{X} = [\bm{x}_1, \ldots, \bm{x}_N]\), where each \(\bm{x}_i \in \mathbb{R}^d\). 
We consider \(K\) mixture components, each with mean \(\bm{\mu}_k \in \mathbb{R}^d\) and covariance \(\bm{\Sigma}_k\). 
Define 
\[
\bm{A}_{ik} 
\;=\; 
\ln \pi_k 
-\tfrac{1}{2}\,(\bm{x}_i - \bm{\mu}_k)^\top \bm{\Sigma}_k^{-1}(\bm{x}_i - \bm{\mu}_k).
\]
The negative log-likelihood (energy) is
\begin{equation}
E^{\mathrm{GMM}}(\bm{X}, \{\bm{\mu}_k\})
\;=\;
-\sum_{i=1}^N 
\ln \Bigl(\sum_{k=1}^K \exp(\bm{A}_{ik})\Bigr).
\label{eq:gmm-energy}
\end{equation}
Minimizing \eqref{eq:gmm-energy} w.r.t.\ each \(\bm{\mu}_k\) recovers the usual EM solution.

\paragraph{Gradient.}
Taking the derivative of \eqref{eq:gmm-energy} w.r.t.\ \(\bm{\mu}_k\) yields
\[
-\frac{\partial E^{\mathrm{GMM}}}{\partial \bm{\mu}_k}
\;=\;
\sum_{i=1}^N 
\text{softmax}_k(\bm{A}_{ik})
\;\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_i - \bm{\mu}_k\bigr),
\]
where 
\(\text{softmax}_k(\bm{A}_{ik}) 
= \tfrac{\exp(\bm{A}_{ik})}{\sum_{u=1}^K \exp(\bm{A}_{iu})}.\)
Setting this gradient to zero gives the standard M-step:
\[
\bm{\mu}_k
\;=\;
\frac{\sum_{i=1}^N \text{softmax}_k(\bm{A}_{ik})\;\bm{x}_i}
     {\sum_{i=1}^N \text{softmax}_k(\bm{A}_{ik})}.
\]

\section{Self-Attention}

\paragraph{Setup.}
Let \(\bm{X} = [\bm{x}_1, \ldots, \bm{x}_N]\), where each \(\bm{x}_i \in \mathbb{R}^d\).  
We define 
\(\bm{Q} = \bm{W}^Q \bm{X}\) and \(\bm{K} = \bm{W}^K \bm{X}\), with \(\bm{W}^Q, \bm{W}^K \in \mathbb{R}^{d\times d}\).  
Denote by \(\bm{q}_c\) the \(c\)-th column of \(\bm{Q}\) and \(\bm{k}_b\) the \(b\)-th column of \(\bm{K}\).  
We define the match scores 
\(\bm{A}_{bc} = \bm{k}_b^\top \bm{q}_c.\)

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{SA}}(\bm{X})
\;=\;
-\sum_{c=1}^N 
\ln \Bigl(\sum_{b=1}^N \exp(\bm{A}_{bc})\Bigr).
\label{eq:sa-energy}
\end{equation}
Minimizing this energy encourages each query \(\bm{q}_c\) to place mass on keys \(\bm{k}_b\) that yield a large dot product.

\paragraph{Gradient.}
Differentiating w.r.t.\ a single token \(\bm{x}_i\), the result is:
\begin{equation}
-\frac{\partial E^{\mathrm{SA}}}{\partial \bm{x}_i}
\;=\;
\sum_{b=1}^N \text{softmax}_b(\bm{A}_{b,i}) 
\bm{W}_Q^\top \bm{W}_K \bm{x}_b
\;+\;
\sum_{c=1}^N \text{softmax}_i(\bm{A}_{i,c}) 
\bm{W}_K^\top \bm{W}_Q \bm{x}_c.
\end{equation}
The first term (``query side'') matches the usual self-attention aggregation form for the token that is acting as a \emph{query}, while the second (``key side'') arises from \(\bm{x}_i\) also contributing as a \emph{key} for other queries.


\section{Hopfield Networks (Softmax Version)}

\paragraph{Setup.}
Let \(\bm{X} = [\bm{x}_1, \ldots, \bm{x}_N]\), each \(\bm{x}_i \in \mathbb{R}^d\).  
We also have memory vectors \(\bm{m}_\mu \in \mathbb{R}^d\), for \(\mu = 1,\ldots,K\).  
Define
\(\bm{A}_{i\mu} = \bm{x}_i^\top \bm{m}_\mu.\)

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Hopfield}}(\bm{X})
\;=\;
-\sum_{i=1}^N
\ln \Bigl(\sum_{\mu=1}^K \exp(\bm{A}_{i\mu})\Bigr).
\label{eq:hopfield-energy}
\end{equation}
Minimizing \eqref{eq:hopfield-energy} encourages each \(\bm{x}_i\) to align with (or be explained by) a subset of the memory vectors \(\bm{m}_\mu\).

\paragraph{Gradient.}
\[
-\frac{\partial E^{\mathrm{Hopfield}}}{\partial \bm{x}_i}
\;=\;
\sum_{\mu=1}^K
\text{softmax}_\mu(\bm{A}_{i\mu})
\;\bm{m}_\mu.
\]
Hence each \(\bm{x}_i\) is updated toward a softmax-weighted combination of the memories.

\section{Slot-Attention}

\paragraph{Setup.}
Let \(\bm{S} = [\bm{s}_1,\ldots,\bm{s}_S]\) be \(S\) latent vectors (slots), each \(\bm{s}_s \in \mathbb{R}^d\).  
We have tokens \(\bm{X} = [\bm{x}_1,\ldots,\bm{x}_N]\), each \(\bm{x}_i \in \mathbb{R}^d\).  
Define 
\(\bm{q}_s = \bm{W}^Q \bm{s}_s\) (queries) and \(\bm{k}_x = \bm{W}^K \bm{x}_x\) (keys).  
Then
\(\bm{A}_{sx} = \bm{q}_s^\top \bm{k}_x.\)

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Slot}}(\bm{S})
\;=\;
-\sum_{s=1}^S
\ln \Bigl(\sum_{x=1}^N \exp(\bm{A}_{sx})\Bigr).
\label{eq:slot-energy}
\end{equation}
Each slot \(\bm{s}_s\) is viewed as a "query" over the token set.

\paragraph{Gradient.}
\begin{equation}
-\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{s}_s}
\;=\;
\sum_{x=1}^N \text{softmax}_x(\bm{A}_{sx}) \bm{W}_Q^\top 
\bm{W}_K \bm{x}_x.
\end{equation}
This matches the iterative update step typically used in slot attention, where each slot aggregates a softmax-weighted combination of tokens.

\section{General Framework}

\paragraph{Setup.}
We consider a collection of nodes (vectors) \(\{\bm{v}_a\}_{a=1}^A\).  Each node \(\bm{v}_a \in \mathbb{R}^d\) can be ``explained by'' a set of parents \(\mathcal{P}(a)\subseteq\{1,\ldots,A\}\), where each parent is another node \(\bm{v}_p\).  We introduce a generic \emph{match} function
\[
\mathrm{match}\bigl(\bm{v}_a,\,\bm{v}_p\bigr),
\]
measuring how well \(\bm{v}_a\) is explained by (or aligns with) \(\bm{v}_p\). For instances,  Dot-product attention: \(\mathrm{match}(\bm{v}_a,\bm{v}_p) = \bm{v}_a^\top \bm{v}_p\) or Gaussian-mixture: \(\mathrm{match}(\bm{v}_a,\bm{v}_p) 
= -\tfrac12\bigl(\bm{v}_a - \bm{v}_p\bigr)^\top \bm{\Sigma}^{-1}\bigl(\bm{v}_a - \bm{v}_p\bigr)\) (plus any log prior).


\paragraph{Energy function.}
Each node \(\bm{v}_a\) has an associated log-sum-exp ``mixture'' term over its parents.  Summing across all nodes yields a single energy:
\begin{equation}
E\bigl(\{\bm{v}_a\}\bigr)
\;=\;
-\sum_{a=1}^A
\ln \Bigl(\sum_{p \,\in\, \mathcal{P}(a)} 
\exp\ \bigl(\mathrm{match} (\bm{v}_a,\bm{v}_p)\bigr)\Bigr).
\label{eq:general-energy}
\end{equation}

\paragraph{Updates}

The total gradient w.r.t.\ \(\bm{v}_a\) decomposes into two sums, as it both explains and is explained by other nodes.
\begin{align}
    -\frac{\partial E}{\partial \bm{v}_a}
    \;=\;&
    \underbrace{\sum_{p \in \mathcal{P}(a)} 
    \text{softmax}_{p}\!\Bigl(\mathrm{match}(\bm{v}_a,\bm{v}_p)\Bigr)
    \;\frac{\partial}{\partial \bm{v}_a}
    \mathrm{match}\bigl(\bm{v}_a,\bm{v}_p\bigr)}_{\text{``being explained by parents''}} \nonumber \\
    &\quad+\;
    \underbrace{\sum_{c :\, a \in \mathcal{P}(c)} 
    \text{softmax}_{a}\!\Bigl(\mathrm{match}(\bm{v}_c,\bm{v}_a)\Bigr)
    \;\frac{\partial}{\partial \bm{v}_a}
    \mathrm{match}\bigl(\bm{v}_c,\bm{v}_a\bigr)}_{\text{``explaining children''}}.
    \label{eq:generic-gradient}
    \end{align}
Here, 
\(\text{softmax}_{p}\!\bigl(\mathrm{match}(\bm{v}_a,\bm{v}_p)\bigr) 
= \frac{\exp(\mathrm{match}(\bm{v}_a,\bm{v}_p))}
       {\sum_{u \in \mathcal{P}(a)} \exp(\mathrm{match}(\bm{v}_a,\bm{v}_u))}\), 
and similarly for \(\text{softmax}_{a}(\mathrm{match}(\bm{v}_c,\bm{v}_a))\).

\paragraph{Proof}
To find the gradient w.r.t.\ a single vector \(\bm{v}_a\), we write
\begin{equation}
\begin{aligned}
\frac{\partial E}{\partial \bm{v}_a}
&=
-\sum_{c=1}^A
\frac{\partial}{\partial \bm{v}_a}
\ln\!\Bigl(\!\sum_{p \in \mathcal{P}(c)} \exp\!\bigl(\mathrm{match}(\bm{v}_c,\bm{v}_p)\bigr)\Bigr) \\[6pt]
&=
-\sum_{c=1}^A
\frac{1}{\sum_{p \in \mathcal{P}(c)} \exp(\mathrm{match}(\bm{v}_c,\bm{v}_p))}
\;\sum_{p \in \mathcal{P}(c)}
\frac{\partial}{\partial \bm{v}_a}
\exp\!\bigl(\mathrm{match}(\bm{v}_c,\bm{v}_p)\bigr).
\end{aligned}
\end{equation}
Applying the chain rule inside the sum:
\begin{equation}
\frac{\partial}{\partial \bm{v}_a}
\exp\!\bigl(\mathrm{match}(\bm{v}_c,\bm{v}_p)\bigr)
=
\exp\!\bigl(\mathrm{match}(\bm{v}_c,\bm{v}_p)\bigr)\,
\frac{\partial}{\partial \bm{v}_a}
\mathrm{match}(\bm{v}_c,\bm{v}_p).
\end{equation}
Hence,
\begin{equation}
\begin{aligned}
-\frac{\partial E}{\partial \bm{v}_a}
&=
\sum_{c=1}^A
\sum_{p \in \mathcal{P}(c)}
\underbrace{
\frac{\exp(\mathrm{match}(\bm{v}_c,\bm{v}_p))}
     {\sum_{u \in \mathcal{P}(c)} \exp(\mathrm{match}(\bm{v}_c,\bm{v}_u))}
}_{\text{softmax}_{p}(\mathrm{match}(\bm{v}_c,\bm{v}_p))}
\;\frac{\partial}{\partial \bm{v}_a}
\mathrm{match}(\bm{v}_c,\bm{v}_p).
\end{aligned}
\end{equation}
Noting that
\(\mathrm{match}(\bm{v}_c,\bm{v}_p)\)
only depends on \(\bm{v}_a\) if \(c=a\) or \(p=a\), we collect these terms:
\begin{equation}
\begin{aligned}
-\frac{\partial E}{\partial \bm{v}_a}
&=
\underbrace{
\sum_{p \in \mathcal{P}(a)}
\text{softmax}_{p}\!\Bigl(\mathrm{match}(\bm{v}_a,\bm{v}_p)\Bigr)\,
\frac{\partial}{\partial \bm{v}_a}
\mathrm{match}(\bm{v}_a,\bm{v}_p)
}_{\text{``explained by parents''}}
\\
&= \underbrace{
\sum_{\substack{c=1\\a \in \mathcal{P}(c)}}^A
\text{softmax}_{a}\!\Bigl(\mathrm{match}(\bm{v}_c,\bm{v}_a)\Bigr)\,
\frac{\partial}{\partial \bm{v}_a}
\mathrm{match}(\bm{v}_c,\bm{v}_a)
}_{\text{``explaining children''}}.
\end{aligned}
\end{equation}
This shows how each \(\bm{v}_a\) is pulled by its own parents and also by the children that depend on it.

\end{document}

\documentclass{article}
\usepackage{amsmath,amssymb,bm}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\begin{document}

\title{Attention via \(\log \sum \exp\) energy}
\author{Alexander Tschantz}
\date{\today}
\maketitle

\section{General Framework}

\paragraph{Setup.}  
We consider a single set of nodes \(\bm{v} = \{\bm{v}_a : a \in \{1, 2, \ldots, A\}\}\), where each node \(\bm{v}_a \in \mathbb{R}^d\). The relationships between these nodes are defined by a set of \(M\) energy functions \(\{E_m : m \in \{1, 2, \ldots, M\}\}\). Each energy function \(E_m\) defines a subset of nodes acting as \emph{children} \(C_m \subseteq \{1, 2, \ldots, A\}\) and a subset acting as \emph{parents} \(P_m \subseteq \{1, 2, \ldots, A\}\), which may overlap.

\paragraph{Energy.}  
Each energy function \(E_m\) defines a similarity function:
\begin{equation}
\mathrm{sim}\bigl(\bm{v}_c, \bm{v}_p\bigr) \quad:\quad \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R},
\end{equation}
which produces a scalar similarity between a child \(\bm{v}_c\) and a parent \(\bm{v}_p\). Using \( \{ \bm{v}_c \} = \{\bm{v}_c : c \in C_m\}\) and \( \{ \bm{v}_p \} = \{\bm{v}_p : p \in P_m\}\), the energy for \(E_m\) is defined as:
\begin{equation}
E_m\bigl(\{ \bm{v}_c \} , \{ \bm{v}_p \} \bigr)
\;=\;
-\sum_{c \in C_m}
\ln \Bigl(
\sum_{p \in P_m}
\exp\bigl(\mathrm{sim}(\bm{v}_c, \bm{v}_p)\bigr)
\Bigr).
\end{equation}
The global energy sums over all energy functions:
\begin{equation}
E\bigl(\{ \bm{v} \} \bigr)
\;=\;
\sum_{m=1}^M
E_m\bigl(\{ \bm{v}_c \} , \{ \bm{v}_p \} \bigr).
\end{equation}

\paragraph{Gradient Updates.}  
For a single node \(\bm{v}_a\), the gradient of the global energy \(E\) w.r.t.\ \(\bm{v}_a\) decomposes into two terms. Let \(\mathcal{M}_c(a) = \{m : a \in C_m\}\) denote the energy functions where \(\bm{v}_a\) acts as a \emph{child}, and \(\mathcal{M}_p(a) = \{m : a \in P_m\}\) the energy functions where \(\bm{v}_a\) acts as a \emph{parent}. Then:
\begin{equation}
\begin{aligned}
-\frac{\partial E}{\partial \bm{v}_a}
\;=\;&
\underbrace{
\sum_{m \in \mathcal{M}_c(a)} \sum_{p \in P_m}
\text{softmax}_{p} \Bigl(\mathrm{sim}(\bm{v}_a, \bm{v}_p)\Bigr)\,
\frac{\partial}{\partial \bm{v}_a}
\mathrm{sim}\bigl(\bm{v}_a, \bm{v}_p\bigr)
}_{\text{\(\bm{v}_a\) acting as a child}} \\
&+
\underbrace{
\sum_{m \in \mathcal{M}_p(a)} \sum_{c \in C_m}
\text{softmax}_{a} \Bigl(\mathrm{sim}(\bm{v}_c, \bm{v}_a)\Bigr)\,
\frac{\partial}{\partial \bm{v}_a}
\mathrm{sim}\bigl(\bm{v}_c, \bm{v}_a\bigr)
}_{\text{\(\bm{v}_a\) acting as a parent}}.
\end{aligned}
\end{equation}
The first term captures contributions from \(\bm{v}_a\) being explained by its parents, while the second term captures contributions from \(\bm{v}_a\) explaining its children.


\section{Gaussian Mixture Models}

\paragraph{Setup.} 
We have \(N\) data points (children) \(\bm{x}_i \in \mathbb{R}^d\), \(i \in C = \{1,\ldots,N\}\), and \(K\) mixture components (parents), each with mean \(\bm{\mu}_k \in \mathbb{R}^d\) and covariance \(\bm{\Sigma}_k\), \(k \in P = \{1,\ldots,K\}\).  Let \(\pi_k\) be the mixing proportion.

\paragraph{Similarity function.}
We define
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{\mu}_k\bigr)
\;=\;
\ln \pi_k
\;-\;
\tfrac12 \bigl(\bm{x}_i - \bm{\mu}_k\bigr)^\top
\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{GMM}}\!\Bigl(\{\bm{x}_i\}, \{\bm{\mu}_k\}\Bigr)
\;=\;
-\sum_{i=1}^N
\ln \Bigl(\sum_{k=1}^K
\exp\bigl(\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
If we differentiate w.r.t.\ \(\bm{\mu}_k\), then
\[
-\frac{\partial E^{\mathrm{GMM}}}{\partial \bm{\mu}_k}
\;=\;
\sum_{i=1}^N 
\text{softmax}_k\!\bigl(\bm{A}_{ik}\bigr)
\;\bm{\Sigma}_k^{-1}\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]
Setting this gradient to zero yields the usual GMM M-step:
\[
\bm{\mu}_k
\;=\;
\frac{\sum_{i=1}^N 
\text{softmax}_k\!\bigl(\bm{A}_{ik}\bigr)\;\bm{x}_i}
     {\sum_{i=1}^N
     \text{softmax}_k\!\bigl(\bm{A}_{ik}\bigr)}.
\]


\section{Cross Attention}

\paragraph{Setup.}
We have a set of child vectors (queries) \(\bm{Q}\in\mathbb{R}^{d\times N_Q}\) and a set of parent vectors (keys) \(\bm{K}\in\mathbb{R}^{d\times N_K}\).  Let
\[
C \;=\;\{1,\ldots,N_Q\}, 
\quad
P \;=\;\{1,\ldots,N_K\},
\]
so \(\bm{v}_c = \bm{q}_c\) is the \(c\)-th query, and \(\bm{v}_p = \bm{k}_p\) is the \(p\)-th key.  Suppose we have learnable weight matrices \(\bm{W}^Q,\bm{W}^K \in \mathbb{R}^{d \times d}\).  Then
\[
\bm{q}_c 
\;=\;
\bm{W}^Q\bm{x}^Q_c,
\quad
\bm{k}_p 
\;=\;
\bm{W}^K\bm{x}^K_p,
\]
where \(\bm{x}^Q_c\) is the raw \(c\)-th query token and \(\bm{x}^K_p\) the raw \(p\)-th key token.

\paragraph{Similarity function.}
\[
\mathrm{sim}\bigl(\bm{q}_c, \bm{k}_p\bigr)
\;=\;
\bm{q}_c^\top \bm{k}_p.
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Cross}}\!\Bigl(\{\bm{q}_c\}, \{\bm{k}_p\}\Bigr)
\;=\;
-\sum_{c=1}^{N_Q}
\ln \Bigl(\sum_{p=1}^{N_K}
\exp\bigl(\bm{q}_c^\top\,\bm{k}_p\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\begin{equation}
-\frac{\partial E^{\mathrm{Cross}}}{\partial \bm{q}_c}
\;=\;
\sum_{p=1}^{N_K}
\text{softmax}_{p}\!\Bigl(\bm{q}_c^\top \bm{k}_p \Bigr)\,
\bm{k}_p.
\end{equation}
\begin{equation}
-\frac{\partial E^{\mathrm{Cross}}}{\partial \bm{k}_p}
\;=\;
\sum_{c=1}^{N_Q}
\text{softmax}_{p}\!\Bigl(\bm{q}_c^\top \bm{k}_p \Bigr)\,
\bm{q}_c.
\end{equation}
When mapping back to the raw tokens \(\bm{x}^Q_c\) or \(\bm{x}^K_p\), chain-rule multiplies by \(\bm{W}^Q\) or \(\bm{W}^K\), respectively.

\section{Hopfield Networks}

\paragraph{Setup.}
We have a set of \emph{children} data vectors \(\bm{x}_i \in \mathbb{R}^d\), \(i\in C=\{1,\ldots,N\}\), and a set of \emph{parent} memory vectors \(\bm{m}_\mu \in \mathbb{R}^d\), \(\mu\in P=\{1,\ldots,K\}\).

\paragraph{Similarity function.}
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{m}_\mu\bigr)
\;=\;
\bm{x}_i^\top \bm{m}_\mu.
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Hopfield}}\!\Bigl(\{\bm{x}_i\}, \{\bm{m}_\mu\}\Bigr)
\;=\;
-\sum_{i=1}^N
\ln \Bigl(\sum_{\mu=1}^K
\exp \bigl(\bm{x}_i^\top \bm{m}_\mu\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\begin{equation}
-\frac{\partial E^{\mathrm{Hopfield}}}{\partial \bm{x}_i}
\;=\;
\sum_{\mu=1}^K
\text{softmax}_{\mu}\!\bigl(\bm{x}_i^\top \bm{m}_\mu\bigr)\,
\bm{m}_\mu.
\end{equation}
\begin{equation}
-\frac{\partial E^{\mathrm{Hopfield}}}{\partial \bm{m}_\mu}
\;=\;
\sum_{i=1}^N
\text{softmax}_{\mu}\!\bigl(\bm{x}_i^\top \bm{m}_\mu\bigr)\,
\bm{x}_i.
\end{equation}

\section{Slot Attention}

\paragraph{Setup.}
Let \(\bm{x}_j \in \mathbb{R}^d\), \(j\in C=\{1,\ldots,N\}\) be the children (tokens), and \(\bm{\mu}_i \in \mathbb{R}^d\), \(i \in P=\{1,\ldots,S\}\) be the parents (slots).  
We typically apply linear transforms \(\bm{W}_K,\bm{W}_Q \in \mathbb{R}^{d \times d}\) to form
\[
\mathrm{sim}\bigl(\bm{x}_j,\bm{\mu}_i\bigr)
\;=\;
\bigl(\bm{W}_K\,\bm{x}_j\bigr)^\top
\bigl(\bm{W}_Q\,\bm{\mu}_i\bigr).
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Slot}}\!\Bigl(\{\bm{x}_j\}, \{\bm{\mu}_i\}\Bigr)
\;=\;
-\sum_{j=1}^N
\ln \Bigl(\sum_{i=1}^S
\exp \bigl(\mathrm{sim}(\bm{x}_j,\bm{\mu}_i)\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\begin{equation}
-\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{x}_j}
\;=\;
\sum_{i=1}^S
\text{softmax}_{i}\!\Bigl(\mathrm{sim}(\bm{x}_j,\bm{\mu}_i)\Bigr)\,
\bm{W}_K^\top \bm{W}_Q\,\bm{\mu}_i.
\end{equation}
\begin{equation}
-\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{\mu}_i}
\;=\;
\sum_{j=1}^N
\text{softmax}_{i}\!\Bigl(\mathrm{sim}(\bm{x}_j,\bm{\mu}_i)\Bigr)\,
\bm{W}_Q^\top \bm{W}_K\,\bm{x}_j.
\end{equation}

\section{Self-Attention}

\paragraph{Setup.}
In self-attention, every node can act as both a child (query) and a parent (key). Concretely, let us have \(N\) tokens \(\{\bm{x}_1,\dots,\bm{x}_N\}\).  We form
\[
\bm{q}_i \;=\; \bm{W}^Q\,\bm{x}_i,
\quad
\bm{k}_i \;=\; \bm{W}^K\,\bm{x}_i,
\]
for \(i = 1,\ldots,N\).  
Thus the set \(C = \{1,\ldots,N\}\) and \(P = \{1,\ldots,N\}\) coincide, with
\[
\mathrm{sim}\bigl(\bm{x}_c,\bm{x}_p\bigr)
\;=\;
\bigl(\bm{W}^Q \bm{x}_c\bigr)^\top
\bigl(\bm{W}^K \bm{x}_p\bigr).
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{SA}}\!\Bigl(\{\bm{x}_i\}\Bigr)
\;=\;
-\sum_{c=1}^N
\ln \Bigl(\sum_{p=1}^N
\exp \Bigl(
(\bm{W}^Q \bm{x}_c)^\top
(\bm{W}^K \bm{x}_p)
\Bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
Since each \(\bm{x}_i\) is \emph{both} a child and a parent, its gradient is a sum of two terms (the child side and the parent side).  Writing it out explicitly:
\begin{equation}
    \begin{aligned}
    -\frac{\partial E^{\mathrm{SA}}}{\partial \bm{x}_i}
    \;=\;&
    \underbrace{
    \sum_{p=1}^N
    \text{softmax}_{p}\!\Bigl(
    (\bm{W}^Q \bm{x}_i)^\top
    (\bm{W}^K \bm{x}_p)
    \Bigr)\,
    \bm{W}_Q^\top \bm{W}_K \,\bm{x}_p
    }_{\text{child $i$ being explained by parents $p$}} \\
    &+
    \underbrace{
    \sum_{c=1}^N
    \text{softmax}_{i}\!\Bigl(
    (\bm{W}^Q \bm{x}_c)^\top
    (\bm{W}^K \bm{x}_i)
    \Bigr)\,
    \bm{W}_K^\top \bm{W}_Q \,\bm{x}_c
    }_{\text{parent $i$ explaining children $c$}}.
    \end{aligned}
    \end{equation}

    \section{Coordinate ascent}
    \label{sec:generic-slot-attention-em}
    
    Consider an energy of the form
    \[
    E(\{\bm{x}_j\}, \{\bm{\mu}_i\}; \theta)
    \;=\;
    -\sum_{j=1}^N
    \ln\Bigl(\sum_{i=1}^S
    \exp\bigl(\mathrm{sim}_\theta(\bm{x}_j,\bm{\mu}_i)\bigr)\Bigr),
    \]
    with \(\theta\) denoting the parameters of the similarity function. In an EM procedure, we alternate:
    \begin{itemize}
        \item \textbf{E-step}: Update latent variables using fixed \(\theta\).
        \item \textbf{M-step}: Update parameters \(\theta\) in closed form given fixed latent assignments.
    \end{itemize}
    
    For \emph{slot attention}, we have:
    \[
    \theta = \{\bm{W}_K,\bm{W}_Q\},
    \] 
    \[
    \mathrm{sim}_\theta(\bm{x}_j,\bm{\mu}_i)
    \;=\;
    (\bm{W}_K\bm{x}_j)^\top(\bm{W}_Q\bm{\mu}_i).
    \]
    \[
    \mathbf{A} 
    \quad\text{with entries}\quad 
    A_{ji}
    \;=\;
    \mathrm{softmax}_i\Bigl(
    (\bm{W}_K\bm{x}_j)^\top(\bm{W}_Q\bm{\mu}_i)
    \Bigr).
    \]
    
    \subsection*{E-step: Update Slots}
    Fix \(\bm{W}_K,\bm{W}_Q\). The gradient for each slot \(\bm{\mu}_i\) is
    \[
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{\mu}_i}
    =
    \sum_{j=1}^N
    A_{ji}\,
    \bm{W}_Q^\top\bm{W}_K\bm{x}_j.
    \]
    Use this gradient to iteratively update \(\{\bm{\mu}_i\}\) until convergence.
    
    \subsection*{M-step: Update Parameters}
    
    Given fixed slots \(\{\bm{\mu}_i\}\) and attention matrix \(\mathbf{A}\), we aim to update the parameters \(\bm{W}_K,\bm{W}_Q\). This is motivated by setting the gradient of the energy with respect to these parameters to zero:
    \[
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{W}_K} = 0,
    \qquad
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{W}_Q} = 0.
    \]
    Under the fixed assignments provided by \(\mathbf{A}\) and slots \(\{\bm{\mu}_i\}\), these conditions are equivalent to solving a weighted least-squares problem. Specifically, we consider minimizing the objective
    \[
    \min_{\bm{W}_K,\bm{W}_Q}
    \sum_{j=1}^N\sum_{i=1}^S
    A_{ji}\,
    \Bigl\|\bm{W}_K\bm{x}_j - \bm{W}_Q\bm{\mu}_i\Bigr\|^2,
    \]
    since the stationary point of this quadratic form corresponds to zero gradients with respect to \(\bm{W}_K,\bm{W}_Q\).


    \paragraph{Update \(\bm{W}_Q\):}
    Differentiate w.r.t.\ \(\bm{W}_Q\), set to zero:
    \[
    \sum_{j,i}A_{ji}\,
    \Bigl(\bm{W}_K\bm{x}_j - \bm{W}_Q\bm{\mu}_i\Bigr)
    \bm{\mu}_i^\top
    \;=\;0,
    \]
    yielding
    \[
    \bm{W}_Q
    \;=\;
    \Biggl(
    \sum_{j,i}A_{ji}\,
    \bm{W}_K\bm{x}_j\,\bm{\mu}_i^\top
    \Biggr)
    \Biggl(\sum_{j,i}A_{ji}\,
    \bm{\mu}_i\bm{\mu}_i^\top
    \Biggr)^{-1}.
    \]
    \paragraph{Update \(\bm{W}_K\):}
    Similarly, differentiate w.r.t.\ \(\bm{W}_K\), set to zero:
    \[
    \sum_{j,i}A_{ji}\,
    \Bigl(\bm{W}_K\bm{x}_j - \bm{W}_Q\bm{\mu}_i\Bigr)
    \bm{x}_j^\top
    \;=\;0,
    \]
    yielding
    \[
    \bm{W}_K
    \;=\;
    \Biggl(
    \sum_{j,i}A_{ji}\,
    \bm{W}_Q\bm{\mu}_i\,\bm{x}_j^\top
    \Biggr)
    \Biggl(\sum_{j,i}A_{ji}\,
    \bm{x}_j\bm{x}_j^\top
    \Biggr)^{-1}.
    \]
    
    \paragraph{Iterate:}
    Alternate between the E-step (updating \(\{\bm{\mu}_i\}\)) and the M-step (updating \(\bm{W}_K,\bm{W}_Q\)) until convergence.


  
    
    
    
    
\clearpage
\appendix

\section{Appendix: Derivation of Gradient Updates}

Let us consider a generic term:
\[
-\ln \Bigl(\,\sum_{m=1}^M \exp \bigl(f_m(\bm{x})\bigr)\Bigr),
\]
where \(\bm{x}\in \mathbb{R}^d\) is some variable, and each \(f_m\) is a scalar function.
We compute its gradient:
\begin{align*}
\frac{\partial}{\partial \bm{x}}
\biggl[
-\ln \Bigl(\,\sum_{m=1}^M \exp \bigl(f_m(\bm{x})\bigr)\Bigr)
\biggr]
&=\;
-\frac{1}{\sum_{m'} \exp \bigl(f_{m'}(\bm{x})\bigr)}
\;\sum_{m=1}^M
\exp \bigl(f_m(\bm{x})\bigr)\;\frac{\partial f_m(\bm{x})}{\partial \bm{x}}
\\[6pt]
&=\;
-\sum_{m=1}^M
\biggl[
\frac{\exp\!\bigl(f_m(\bm{x})\bigr)}{\sum_{m'} \exp \bigl(f_{m'}(\bm{x})\bigr)}
\biggr]
\;\frac{\partial f_m(\bm{x})}{\partial \bm{x}}.
\end{align*}
Defining
\(\mathrm{softmax}_m\bigl(f(\bm{x})\bigr) 
= 
\tfrac{\exp(f_m(\bm{x}))}{\sum_{m'} \exp(f_{m'}(\bm{x}))},\)
this is
\[
-\sum_{m=1}^M
\mathrm{softmax}_m \bigl(f(\bm{x})\bigr)
\;\frac{\partial f_m(\bm{x})}{\partial \bm{x}},
\]
which matches the \(\mathrm{softmax}\)-weighted gradient structure.


\end{document}

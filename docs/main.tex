


\documentclass{article}
\usepackage{amsmath,amssymb,bm}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\begin{document}

\title{Attention via \(\log \sum \exp\) energy}
\author{Alexander Tschantz}
\date{\today}
\maketitle

\section{General Framework}

\paragraph{Setup.}  
We consider a single set of nodes \(\bm{v} = \{\bm{v}_a : a \in \{1, 2, \ldots, A\}\}\), where each node \(\bm{v}_a \in \mathbb{R}^d\). The relationships between these nodes are defined by a set of \(M\) energy functions \(\{E_m : m \in \{1, 2, \ldots, M\}\}\). Each energy function \(E_m\) defines a subset of nodes acting as \emph{children} \(C_m \subseteq \{1, 2, \ldots, A\}\) and a subset acting as \emph{parents} \(P_m \subseteq \{1, 2, \ldots, A\}\), which may overlap.

\paragraph{Energy.}  
Each energy function \(E_m\) defines a similarity function:
\begin{equation}
\mathrm{sim}\bigl(\bm{v}_c, \bm{v}_p\bigr) \quad:\quad \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R},
\end{equation}
which produces a scalar similarity between a child \(\bm{v}_c\) and a parent \(\bm{v}_p\). Using \( \{ \bm{v}_c \} = \{\bm{v}_c : c \in C_m\}\) and \( \{ \bm{v}_p \} = \{\bm{v}_p : p \in P_m\}\), the energy for \(E_m\) is defined as:
\begin{equation}
E_m\bigl(\{ \bm{v}_c \} , \{ \bm{v}_p \} \bigr)
\;=\;
-\sum_{c \in C_m}
\ln \Bigl(
\sum_{p \in P_m}
\exp\bigl(\mathrm{sim}(\bm{v}_c, \bm{v}_p)\bigr)
\Bigr).
\end{equation}
The global energy sums over all energy functions:
\begin{equation}
E\bigl(\{ \bm{v} \} \bigr)
\;=\;
\sum_{m=1}^M
E_m\bigl(\{ \bm{v}_c \} , \{ \bm{v}_p \} \bigr).
\end{equation}

\paragraph{Gradient Updates.}  
For a single node \(\bm{v}_a\), the gradient of the global energy \(E\) w.r.t.\ \(\bm{v}_a\) decomposes into two terms. Let \(\mathcal{M}_c(a) = \{m : a \in C_m\}\) denote the energy functions where \(\bm{v}_a\) acts as a \emph{child}, and \(\mathcal{M}_p(a) = \{m : a \in P_m\}\) the energy functions where \(\bm{v}_a\) acts as a \emph{parent}. Then:
\begin{equation}
\begin{aligned}
-\frac{\partial E}{\partial \bm{v}_a}
\;=\;&
\underbrace{
\sum_{m \in \mathcal{M}_c(a)} \sum_{p \in P_m}
\text{softmax}_{p} \Bigl(\mathrm{sim}(\bm{v}_a, \bm{v}_p)\Bigr)\,
\frac{\partial}{\partial \bm{v}_a}
\mathrm{sim}\bigl(\bm{v}_a, \bm{v}_p\bigr)
}_{\text{\(\bm{v}_a\) acting as a child}} \\
&+
\underbrace{
\sum_{m \in \mathcal{M}_p(a)} \sum_{c \in C_m}
\text{softmax}_{a} \Bigl(\mathrm{sim}(\bm{v}_c, \bm{v}_a)\Bigr)\,
\frac{\partial}{\partial \bm{v}_a}
\mathrm{sim}\bigl(\bm{v}_c, \bm{v}_a\bigr)
}_{\text{\(\bm{v}_a\) acting as a parent}}.
\end{aligned}
\end{equation}
The first term captures contributions from \(\bm{v}_a\) being explained by its parents, while the second term captures contributions from \(\bm{v}_a\) explaining its children.


\section{Gaussian Mixture Models}

\paragraph{Setup.} 
We have \(N\) data points (children) \(\bm{x}_i \in \mathbb{R}^d\), \(i \in C = \{1,\ldots,N\}\), and \(K\) mixture components (parents), each with mean \(\bm{\mu}_k \in \mathbb{R}^d\) and covariance \(\bm{\Sigma}_k\), \(k \in P = \{1,\ldots,K\}\).  Let \(\pi_k\) be the mixing proportion.

\paragraph{Similarity function.}
We define
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{\mu}_k\bigr)
\;=\;
\ln \pi_k
\;-\;
\tfrac12 \bigl(\bm{x}_i - \bm{\mu}_k\bigr)^\top
\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{GMM}}\!\Bigl(\{\bm{x}_i\}, \{\bm{\mu}_k\}\Bigr)
\;=\;
-\sum_{i=1}^N
\ln \Bigl(\sum_{k=1}^K
\exp\bigl(\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
If we differentiate w.r.t.\ \(\bm{\mu}_k\), then
\[
-\frac{\partial E^{\mathrm{GMM}}}{\partial \bm{\mu}_k}
\;=\;
\sum_{i=1}^N 
\text{softmax}_k\!\bigl(\bm{A}_{ik}\bigr)
\;\bm{\Sigma}_k^{-1}\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]
Setting this gradient to zero yields the usual GMM M-step:
\[
\bm{\mu}_k
\;=\;
\frac{\sum_{i=1}^N 
\text{softmax}_k\!\bigl(\bm{A}_{ik}\bigr)\;\bm{x}_i}
     {\sum_{i=1}^N
     \text{softmax}_k\!\bigl(\bm{A}_{ik}\bigr)}.
\]


\section{Cross Attention}

\paragraph{Setup.}
We have a set of child vectors (queries) \(\bm{Q}\in\mathbb{R}^{d\times N_Q}\) and a set of parent vectors (keys) \(\bm{K}\in\mathbb{R}^{d\times N_K}\).  Let
\[
C \;=\;\{1,\ldots,N_Q\}, 
\quad
P \;=\;\{1,\ldots,N_K\},
\]
so \(\bm{v}_c = \bm{q}_c\) is the \(c\)-th query, and \(\bm{v}_p = \bm{k}_p\) is the \(p\)-th key.  Suppose we have learnable weight matrices \(\bm{W}^Q,\bm{W}^K \in \mathbb{R}^{d \times d}\).  Then
\[
\bm{q}_c 
\;=\;
\bm{W}^Q\bm{x}^Q_c,
\quad
\bm{k}_p 
\;=\;
\bm{W}^K\bm{x}^K_p,
\]
where \(\bm{x}^Q_c\) is the raw \(c\)-th query token and \(\bm{x}^K_p\) the raw \(p\)-th key token.

\paragraph{Similarity function.}
\[
\mathrm{sim}\bigl(\bm{q}_c, \bm{k}_p\bigr)
\;=\;
\bm{q}_c^\top \bm{k}_p.
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Cross}}\!\Bigl(\{\bm{q}_c\}, \{\bm{k}_p\}\Bigr)
\;=\;
-\sum_{c=1}^{N_Q}
\ln \Bigl(\sum_{p=1}^{N_K}
\exp\bigl(\bm{q}_c^\top\,\bm{k}_p\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\begin{equation}
-\frac{\partial E^{\mathrm{Cross}}}{\partial \bm{q}_c}
\;=\;
\sum_{p=1}^{N_K}
\text{softmax}_{p}\!\Bigl(\bm{q}_c^\top \bm{k}_p \Bigr)\,
\bm{k}_p.
\end{equation}
\begin{equation}
-\frac{\partial E^{\mathrm{Cross}}}{\partial \bm{k}_p}
\;=\;
\sum_{c=1}^{N_Q}
\text{softmax}_{p}\!\Bigl(\bm{q}_c^\top \bm{k}_p \Bigr)\,
\bm{q}_c.
\end{equation}
When mapping back to the raw tokens \(\bm{x}^Q_c\) or \(\bm{x}^K_p\), chain-rule multiplies by \(\bm{W}^Q\) or \(\bm{W}^K\), respectively.

\section{Hopfield Networks}

\paragraph{Setup.}
We have a set of \emph{children} data vectors \(\bm{x}_i \in \mathbb{R}^d\), \(i\in C=\{1,\ldots,N\}\), and a set of \emph{parent} memory vectors \(\bm{m}_\mu \in \mathbb{R}^d\), \(\mu\in P=\{1,\ldots,K\}\).

\paragraph{Similarity function.}
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{m}_\mu\bigr)
\;=\;
\bm{x}_i^\top \bm{m}_\mu.
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Hopfield}}\!\Bigl(\{\bm{x}_i\}, \{\bm{m}_\mu\}\Bigr)
\;=\;
-\sum_{i=1}^N
\ln \Bigl(\sum_{\mu=1}^K
\exp \bigl(\bm{x}_i^\top \bm{m}_\mu\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\begin{equation}
-\frac{\partial E^{\mathrm{Hopfield}}}{\partial \bm{x}_i}
\;=\;
\sum_{\mu=1}^K
\text{softmax}_{\mu}\!\bigl(\bm{x}_i^\top \bm{m}_\mu\bigr)\,
\bm{m}_\mu.
\end{equation}
\begin{equation}
-\frac{\partial E^{\mathrm{Hopfield}}}{\partial \bm{m}_\mu}
\;=\;
\sum_{i=1}^N
\text{softmax}_{\mu}\!\bigl(\bm{x}_i^\top \bm{m}_\mu\bigr)\,
\bm{x}_i.
\end{equation}

\section{Slot Attention}

\paragraph{Setup.}
Let \(\bm{x}_j \in \mathbb{R}^d\), \(j\in C=\{1,\ldots,N\}\) be the children (tokens), and \(\bm{\mu}_i \in \mathbb{R}^d\), \(i \in P=\{1,\ldots,S\}\) be the parents (slots).  
We typically apply linear transforms \(\bm{W}_K,\bm{W}_Q \in \mathbb{R}^{d \times d}\) to form
\[
\mathrm{sim}\bigl(\bm{x}_j,\bm{\mu}_i\bigr)
\;=\;
\bigl(\bm{W}_K\,\bm{x}_j\bigr)^\top
\bigl(\bm{W}_Q\,\bm{\mu}_i\bigr).
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{Slot}}\!\Bigl(\{\bm{x}_j\}, \{\bm{\mu}_i\}\Bigr)
\;=\;
-\sum_{j=1}^N
\ln \Bigl(\sum_{i=1}^S
\exp \bigl(\mathrm{sim}(\bm{x}_j,\bm{\mu}_i)\bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\begin{equation}
-\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{x}_j}
\;=\;
\sum_{i=1}^S
\text{softmax}_{i}\!\Bigl(\mathrm{sim}(\bm{x}_j,\bm{\mu}_i)\Bigr)\,
\bm{W}_K^\top \bm{W}_Q\,\bm{\mu}_i.
\end{equation}
\begin{equation}
-\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{\mu}_i}
\;=\;
\sum_{j=1}^N
\text{softmax}_{i}\!\Bigl(\mathrm{sim}(\bm{x}_j,\bm{\mu}_i)\Bigr)\,
\bm{W}_Q^\top \bm{W}_K\,\bm{x}_j.
\end{equation}

\section{Self-Attention}

\paragraph{Setup.}
In self-attention, every node can act as both a child (query) and a parent (key). Concretely, let us have \(N\) tokens \(\{\bm{x}_1,\dots,\bm{x}_N\}\).  We form
\[
\bm{q}_i \;=\; \bm{W}^Q\,\bm{x}_i,
\quad
\bm{k}_i \;=\; \bm{W}^K\,\bm{x}_i,
\]
for \(i = 1,\ldots,N\).  
Thus the set \(C = \{1,\ldots,N\}\) and \(P = \{1,\ldots,N\}\) coincide, with
\[
\mathrm{sim}\bigl(\bm{x}_c,\bm{x}_p\bigr)
\;=\;
\bigl(\bm{W}^Q \bm{x}_c\bigr)^\top
\bigl(\bm{W}^K \bm{x}_p\bigr).
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{SA}}\!\Bigl(\{\bm{x}_i\}\Bigr)
\;=\;
-\sum_{c=1}^N
\ln \Bigl(\sum_{p=1}^N
\exp \Bigl(
(\bm{W}^Q \bm{x}_c)^\top
(\bm{W}^K \bm{x}_p)
\Bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
Since each \(\bm{x}_i\) is \emph{both} a child and a parent, its gradient is a sum of two terms (the child side and the parent side).  Writing it out explicitly:
\begin{equation}
    \begin{aligned}
    -\frac{\partial E^{\mathrm{SA}}}{\partial \bm{x}_i}
    \;=\;&
    \underbrace{
    \sum_{p=1}^N
    \text{softmax}_{p}\!\Bigl(
    (\bm{W}^Q \bm{x}_i)^\top
    (\bm{W}^K \bm{x}_p)
    \Bigr)\,
    \bm{W}_Q^\top \bm{W}_K \,\bm{x}_p
    }_{\text{child $i$ being explained by parents $p$}} \\
    &+
    \underbrace{
    \sum_{c=1}^N
    \text{softmax}_{i}\!\Bigl(
    (\bm{W}^Q \bm{x}_c)^\top
    (\bm{W}^K \bm{x}_i)
    \Bigr)\,
    \bm{W}_K^\top \bm{W}_Q \,\bm{x}_c
    }_{\text{parent $i$ explaining children $c$}}.
    \end{aligned}
    \end{equation}


\section{Predictive coding}

\paragraph{Setup.}
We again have child vectors \(\{\bm{x}_i\}_{i=1}^N\) and a set of parent \(\bm{\mu}_k \in \mathbb{R}^d, k=1,\dots,K\).  However, rather than a direct difference \(\bm{x}_i-\bm{\mu}_k\), let us assume the \emph{model} maps \(\bm{\mu}_k\) through some non-linear function \(f_\phi(\cdot)\) before comparing to \(\bm{x}_i\).  For instance, \(f_\phi\) could be a neural network.

\paragraph{Similarity function.}
Define
\[
\mathrm{sim}\bigl(\bm{x}_i,\bm{\mu}_k\bigr)
\;=\;
-\,\tfrac12\,
\bigl\|\bm{x}_i \;-\; f_\phi(\bm{\mu}_k)\bigr\|^2.
\]

\paragraph{Energy.}
\begin{equation}
E^{\mathrm{PC}}\!\Bigl(\{\bm{x}_i\}, \{\bm{\mu}_k\}\Bigr)
\;=\;
-\sum_{i=1}^N
\ln \Bigl(\sum_{k=1}^K
\exp\Bigl(-\tfrac12\,\bigl\|\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr\|^2\Bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
\[
\alpha_{i,k}
\;=\;
\mathrm{softmax}_k\Bigl(
-\,\tfrac12\,\bigl\|\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr\|^2
\Bigr).
\]

\[
-\frac{\partial E^{\mathrm{PC}}}{\partial \bm{x}_i}
\;=\;
\sum_{k=1}^K 
\alpha_{i,k}\,\bigl(\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr).
\]
\[
-\frac{\partial E^{\mathrm{PC}}}{\partial \bm{\mu}_k}
\;=\;
\sum_{i=1}^N 
\alpha_{i,k}\,
\underbrace{
\frac{\partial f_\phi(\bm{\mu}_k)}{\partial \bm{\mu}_k}
}_{\text{Jacobian of }f_\phi}
\;\bigl(\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr).
\]
Here, \(\tfrac{\partial f_\phi(\bm{\mu}_k)}{\partial \bm{\mu}_k}\) is the \(d\times d\) Jacobian (or more general shape if \(\bm{\mu}_k\) and \(f_\phi(\bm{\mu}_k)\) differ in dimension). 


\section{Switching Linear Dynamical System}

\paragraph{Setup.}
Consider a time series of observations \(\{\bm{x}_t\}_{t=1}^T\), where each \(\bm{x}_t \in \mathbb{R}^d\).  
We assume there are \(K\) distinct (linear) dynamical modes, each with parameters \(\{\bm{A}_k, \bm{b}_k, \bm{\Sigma}_k\}\). Let \(\pi_k\) be the mixing weight of mode \(k\).  A typical \emph{switching linear dynamical system} (SLDS) posits:
\[
\bm{x}_{t+1}
\;\approx\;
\bm{A}_k\,\bm{x}_t \;+\; \bm{b}_k,
\quad
k \in \{1,\dots,K\},
\]
with Gaussian noise \(\bm{\Sigma}_k\).  
We treat \(\bm{x}_{t+1}\) as a \emph{child} and the \(\{\bm{A}_k, \bm{b}_k\}\) (together with \(\bm{x}_t\)) as \emph{parents} in a mixture-of-linear-dynamics fashion.

\paragraph{Similarity function.}
Define, for each mode \(k\),
\[
\mathrm{sim}\bigl(\bm{x}_{t+1}, \bm{x}_t; \bm{A}_k, \bm{b}_k\bigr)
\;=\;
\ln \pi_k
\;-\;
\tfrac12\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)^\top
\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]

\paragraph{Energy.}
Summing over all time steps \(t = 1,\ldots,T-1\), we write
\begin{equation}
E^{\mathrm{SLDS}}\!\Bigl(\{\bm{x}_t\}, \{\bm{A}_k, \bm{b}_k\}\Bigr)
\;=\;
-\sum_{t=1}^{T-1}\,
\ln \Bigl(\sum_{k=1}^K
\exp\!\Bigl(
\mathrm{sim}\bigl(\bm{x}_{t+1}, \bm{x}_t; \bm{A}_k, \bm{b}_k\bigr)
\Bigr)\Bigr).
\end{equation}

\paragraph{Gradients.}
Let 
\[
\alpha_{t,k} 
\;=\;
\mathrm{softmax}_{k}\Bigl(
\mathrm{sim}\bigl(\bm{x}_{t+1}, \bm{x}_t; \bm{A}_k, \bm{b}_k\bigr)\Bigr),
\]
i.e.\ the normalized exponent for mode \(k\).  

\[
-\frac{\partial E^{\mathrm{SLDS}}}{\partial \bm{x}_{t+1}}
\;=\;
\sum_{k=1}^K 
\alpha_{t,k}\,\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]
\[
-\frac{\partial E^{\mathrm{SLDS}}}{\partial \bm{x}_t}
\;=\;
\sum_{k=1}^K 
\alpha_{t,k}\,
\bigl(-\,\bm{A}_k^\top\,\bm{\Sigma}_k^{-1}\bigr)\,
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)
\quad (t=1,\dots,T-1).
\]
\[
-\frac{\partial E^{\mathrm{SLDS}}}{\partial \bm{A}_k}
\;=\;
\sum_{t=1}^{T-1}
\alpha_{t,k}\,\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)\,\bm{x}_t^\top.
\]
\[
-\frac{\partial E^{\mathrm{SLDS}}}{\partial \bm{b}_k}
\;=\;
\sum_{t=1}^{T-1}
\alpha_{t,k}\,\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]

\section{Layer Normalization}

\paragraph{Setup.}
We consider a batch of input vectors \(\{\bm{x}_i\}_{i=1}^B\), where each vector \(\bm{x}_i \in \mathbb{R}^D\). Each vector is normalized by subtracting the mean and dividing by the standard deviation, with learnable scaling and bias parameters \(\gamma, \delta \in \mathbb{R}^D\). For numerical stability, a small constant \(\epsilon > 0\) is added to the variance.

\paragraph{Energy.}
The energy for layer normalization is given by:
\[
E^{\mathrm{LN}}(\{\bm{x}_i\}) = \sum_{i=1}^B \left[ \gamma \sqrt{\frac{1}{D} \sum_{j=1}^D (x_{ij} - \bar{\bm{x}}_i)^2 + \epsilon} + \sum_{j=1}^D \delta_j x_{ij} \right],
\]
where:
\[
\bar{\bm{x}}_i = \frac{1}{D} \sum_{j=1}^D x_{ij}.
\]

\paragraph{Derivative.}
The normalized outputs are obtained as the derivative of the energy with respect to the inputs:
\[
\frac{\partial E^{\mathrm{LN}}}{\partial x_{ij}} = \gamma_j \frac{x_{ij} - \bar{\bm{x}}_i}{\sqrt{\frac{1}{D} \sum_{k=1}^D (x_{ik} - \bar{\bm{x}}_i)^2 + \epsilon}} + \delta_j.
\]
\[
\bar{\bm{x}}_i = \frac{1}{D} \sum_{k=1}^D x_{ik}.
\]

\section{Spatiotemporal Self-Attention}
\label{sec:spatiotemporal-attention}

\paragraph{Setup.}
We consider \(K\) slots (objects) observed over \(T\) time steps.  
Each slot \(k \in \{1,\dots,K\}\) at time \(t \in \{1,\dots,T\}\) is indexed by \(n = (k,t)\), resulting in a total of \(N = K \times T\) tokens.  
Let \(\bm{x}_n \in \mathbb{R}^d\) represent the feature vector of token \(n\).

For each token \(n = (k,t)\), define its parent sets as:
\[
\mathcal{P}_n^{\mathrm{slot}} = \{ m = (k', t) \mid k' \neq k \}, \quad \mathcal{P}_n^{\mathrm{time}} = \{ m = (k, t') \mid t' < t \}.
\]
Thus, each token attends to other slots at the same time step and to its own past states.

\paragraph{Energy.}
For each token \(n\), the similarity to a parent \(m\) is defined as:
\[
\mathrm{sim}(\bm{x}_n, \bm{x}_m) =
\begin{cases}
\bm{q}_{n}^{\mathrm{slot}\top} \bm{k}_{m}^{\mathrm{slot}}, & \text{if } m \in \mathcal{P}_n^{\mathrm{slot}}, \\
\bm{q}_{n}^{\mathrm{time}\top} \bm{k}_{m}^{\mathrm{time}}, & \text{if } m \in \mathcal{P}_n^{\mathrm{time}}, \\
-\infty, & \text{otherwise},
\end{cases}
\]
where
\[
\bm{q}_{n}^{\mathrm{slot}} = \bm{W}_k^{Q,\mathrm{slot}} \bm{x}_n, \quad \bm{k}_{m}^{\mathrm{slot}} = \bm{W}_{k'}^{K,\mathrm{slot}} \bm{x}_m,
\]
\[
\bm{q}_{n}^{\mathrm{time}} = \bm{W}_k^{Q,\mathrm{time}} \bm{x}_n, \quad \bm{k}_{m}^{\mathrm{time}} = \bm{W}_k^{K,\mathrm{time}} \bm{x}_m.
\]
The energy for token \(n\) is then:
\[
E_n = -\ln \left( \sum_{m \in \mathcal{P}_n^{\mathrm{slot}}} \exp\bigl(\mathrm{sim}(\bm{x}_n, \bm{x}_m)\bigr) + \sum_{m \in \mathcal{P}_n^{\mathrm{time}}} \exp\bigl(\mathrm{sim}(\bm{x}_n, \bm{x}_m)\bigr) \right).
\]

\paragraph{Derivatives.}
Let \(\mathbf{A} \in \mathbb{R}^{N \times N}\) be the attention matrix with entries
\[
A_{n,m} = \alpha_{n \to m} = \mathrm{softmax}_m\bigl(\mathrm{sim}(\bm{x}_n, \bm{x}_m)\bigr).
\]
For a specific token \(n\), the gradient with respect to \(\bm{x}_n\) is:
\begin{equation}
\label{eq:grad_xn_spatiotemporal}
\begin{aligned}
\frac{\partial E_n}{\partial \bm{x}_n}
\;=\;&
\sum_{m \in \mathcal{P}_n^{\mathrm{slot}}} \alpha_{n \to m} \, \bm{W}_k^{Q,\mathrm{slot}\top} \bm{W}_{k'}^{K,\mathrm{slot}} \bm{x}_m \\
&+
\sum_{m \in \mathcal{P}_n^{\mathrm{time}}} \alpha_{n \to m} \, \bm{W}_k^{Q,\mathrm{time}\top} \bm{W}_k^{K,\mathrm{time}} \bm{x}_m.
\end{aligned}
\end{equation}
Here, the first term corresponds to \emph{inter-slot} attention and the second term corresponds to \emph{inter-time} attention.

\paragraph{Remark (Full Version).}
Allowing tokens to attend to all slots across all time steps enables comprehensive interactions and temporal dependencies but increases computational complexity.


\section{Coordinate ascent}
\label{sec:generic-slot-attention-em}
    
    Consider an energy of the form
    \[
    E(\{\bm{x}_j\}, \{\bm{\mu}_i\}; \theta)
    \;=\;
    -\sum_{j=1}^N
    \ln\Bigl(\sum_{i=1}^S
    \exp\bigl(\mathrm{sim}_\theta(\bm{x}_j,\bm{\mu}_i)\bigr)\Bigr),
    \]
    with \(\theta\) denoting the parameters of the similarity function. In an EM procedure, we alternate:
    \begin{itemize}
        \item \textbf{E-step}: Update latent variables using fixed \(\theta\).
        \item \textbf{M-step}: Update parameters \(\theta\) in closed form given fixed latent assignments.
    \end{itemize}
    
    For \emph{slot attention}, we have:
    \[
    \theta = \{\bm{W}_K,\bm{W}_Q\},
    \] 
    \[
    \mathrm{sim}_\theta(\bm{x}_j,\bm{\mu}_i)
    \;=\;
    (\bm{W}_K\bm{x}_j)^\top(\bm{W}_Q\bm{\mu}_i).
    \]
    \[
    \mathbf{A} 
    \quad\text{with entries}\quad 
    A_{ji}
    \;=\;
    \mathrm{softmax}_i\Bigl(
    (\bm{W}_K\bm{x}_j)^\top(\bm{W}_Q\bm{\mu}_i)
    \Bigr).
    \]
    
    \subsection*{E-step: Update Slots}
    Fix \(\bm{W}_K,\bm{W}_Q\). The gradient for each slot \(\bm{\mu}_i\) is
    \[
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{\mu}_i}
    =
    \sum_{j=1}^N
    A_{ji}\,
    \bm{W}_Q^\top\bm{W}_K\bm{x}_j.
    \]
    Use this gradient to iteratively update \(\{\bm{\mu}_i\}\) until convergence.
    
    \subsection*{M-step: Update Parameters}
    
    Given fixed slots \(\{\bm{\mu}_i\}\) and attention matrix \(\mathbf{A}\), we aim to update the parameters \(\bm{W}_K,\bm{W}_Q\). This is motivated by setting the gradient of the energy with respect to these parameters to zero:
    \[
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{W}_K} = 0,
    \qquad
    -\frac{\partial E^{\mathrm{Slot}}}{\partial \bm{W}_Q} = 0.
    \]
    Under the fixed assignments provided by \(\mathbf{A}\) and slots \(\{\bm{\mu}_i\}\), these conditions are equivalent to solving a weighted least-squares problem. Specifically, we consider minimizing the objective
    \[
    \min_{\bm{W}_K,\bm{W}_Q}
    \sum_{j=1}^N\sum_{i=1}^S
    A_{ji}\,
    \Bigl\|\bm{W}_K\bm{x}_j - \bm{W}_Q\bm{\mu}_i\Bigr\|^2,
    \]
    since the stationary point of this quadratic form corresponds to zero gradients with respect to \(\bm{W}_K,\bm{W}_Q\).


    \paragraph{Update \(\bm{W}_Q\):}
    Differentiate w.r.t.\ \(\bm{W}_Q\), set to zero:
    \[
    \sum_{j,i}A_{ji}\,
    \Bigl(\bm{W}_K\bm{x}_j - \bm{W}_Q\bm{\mu}_i\Bigr)
    \bm{\mu}_i^\top
    \;=\;0,
    \]
    yielding
    \[
    \bm{W}_Q
    \;=\;
    \Biggl(
    \sum_{j,i}A_{ji}\,
    \bm{W}_K\bm{x}_j\,\bm{\mu}_i^\top
    \Biggr)
    \Biggl(\sum_{j,i}A_{ji}\,
    \bm{\mu}_i\bm{\mu}_i^\top
    \Biggr)^{-1}.
    \]
    \paragraph{Update \(\bm{W}_K\):}
    Similarly, differentiate w.r.t.\ \(\bm{W}_K\), set to zero:
    \[
    \sum_{j,i}A_{ji}\,
    \Bigl(\bm{W}_K\bm{x}_j - \bm{W}_Q\bm{\mu}_i\Bigr)
    \bm{x}_j^\top
    \;=\;0,
    \]
    yielding
    \[
    \bm{W}_K
    \;=\;
    \Biggl(
    \sum_{j,i}A_{ji}\,
    \bm{W}_Q\bm{\mu}_i\,\bm{x}_j^\top
    \Biggr)
    \Biggl(\sum_{j,i}A_{ji}\,
    \bm{x}_j\bm{x}_j^\top
    \Biggr)^{-1}.
    \]
    
    \paragraph{Iterate:}
    Alternate between the E-step (updating \(\{\bm{\mu}_i\}\)) and the M-step (updating \(\bm{W}_K,\bm{W}_Q\)) until convergence.


  
    
    
\clearpage
\appendix

\section{Appendix: Derivation of Gradient Updates}

Let us consider a generic term:
\[
-\ln \Bigl(\,\sum_{m=1}^M \exp \bigl(f_m(\bm{x})\bigr)\Bigr),
\]
where \(\bm{x}\in \mathbb{R}^d\) is some variable, and each \(f_m\) is a scalar function.
We compute its gradient:
\begin{align*}
\frac{\partial}{\partial \bm{x}}
\biggl[
-\ln \Bigl(\,\sum_{m=1}^M \exp \bigl(f_m(\bm{x})\bigr)\Bigr)
\biggr]
&=\;
-\frac{1}{\sum_{m'} \exp \bigl(f_{m'}(\bm{x})\bigr)}
\;\sum_{m=1}^M
\exp \bigl(f_m(\bm{x})\bigr)\;\frac{\partial f_m(\bm{x})}{\partial \bm{x}}
\\[6pt]
&=\;
-\sum_{m=1}^M
\biggl[
\frac{\exp\!\bigl(f_m(\bm{x})\bigr)}{\sum_{m'} \exp \bigl(f_{m'}(\bm{x})\bigr)}
\biggr]
\;\frac{\partial f_m(\bm{x})}{\partial \bm{x}}.
\end{align*}
Defining
\(\mathrm{softmax}_m\bigl(f(\bm{x})\bigr) 
= 
\tfrac{\exp(f_m(\bm{x}))}{\sum_{m'} \exp(f_{m'}(\bm{x}))},\)
this is
\[
-\sum_{m=1}^M
\mathrm{softmax}_m \bigl(f(\bm{x})\bigr)
\;\frac{\partial f_m(\bm{x})}{\partial \bm{x}},
\]
which matches the \(\mathrm{softmax}\)-weighted gradient structure.


\section{Appendix}
\label{sec:deriv-sim-models}

Here, we collect the explicit partial derivatives of \(\mathrm{sim}\) for each model discussed.

\subsection*{Gaussian Mixture Models}
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{\mu}_k\bigr)
\;=\;
\ln \pi_k
\;-\;
\tfrac12 \bigl(\bm{x}_i - \bm{\mu}_k\bigr)^\top
\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]
\[
\frac{\partial}{\partial \bm{\mu}_k} \mathrm{sim}(\bm{x}_i,\bm{\mu}_k)
\;=\;
\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_i - \bm{\mu}_k\bigr).
\]

\subsection*{Cross Attention}
\[
\mathrm{sim}\bigl(\bm{q}_c, \bm{k}_p\bigr)
\;=\;
\bm{q}_c^\top \bm{k}_p.
\]
\[
\frac{\partial}{\partial \bm{q}_c} \mathrm{sim}(\bm{q}_c,\bm{k}_p)
\;=\;
\bm{k}_p,
\quad
\frac{\partial}{\partial \bm{k}_p} \mathrm{sim}(\bm{q}_c,\bm{k}_p)
\;=\;
\bm{q}_c.
\]

\subsection*{Hopfield Networks}
\[
\mathrm{sim}\bigl(\bm{x}_i, \bm{m}_\mu\bigr)
\;=\;
\bm{x}_i^\top \bm{m}_\mu.
\]
\[
\frac{\partial}{\partial \bm{x}_i} \mathrm{sim}(\bm{x}_i,\bm{m}_\mu)
\;=\;
\bm{m}_\mu,
\quad
\frac{\partial}{\partial \bm{m}_\mu} \mathrm{sim}(\bm{x}_i,\bm{m}_\mu)
\;=\;
\bm{x}_i.
\]

\subsection*{Slot Attention}
\[
\mathrm{sim}\bigl(\bm{x}_j,\bm{\mu}_i\bigr)
\;=\;
(\bm{W}_K\,\bm{x}_j)^\top
(\bm{W}_Q\,\bm{\mu}_i).
\]

\[
\frac{\partial}{\partial \bm{x}_j} \mathrm{sim}(\bm{x}_j,\bm{\mu}_i)
\;=\;
\bm{W}_K^\top \,\bm{W}_Q\,\bm{\mu}_i,
\quad
\frac{\partial}{\partial \bm{\mu}_i} \mathrm{sim}(\bm{x}_j,\bm{\mu}_i)
\;=\;
\bm{W}_Q^\top \,\bm{W}_K\,\bm{x}_j.
\]

\subsection*{Self-Attention}
\[
\mathrm{sim}\bigl(\bm{x}_c,\bm{x}_p\bigr)
\;=\;
(\bm{W}^Q \bm{x}_c)^\top\,(\bm{W}^K \bm{x}_p).
\]
\[
\frac{\partial}{\partial \bm{x}_c} \mathrm{sim}(\bm{x}_c,\bm{x}_p)
\;=\;
{\bm{W}^Q}^\top\,\bm{W}^K\,\bm{x}_p,
\quad
\frac{\partial}{\partial \bm{x}_p} \mathrm{sim}(\bm{x}_c,\bm{x}_p)
\;=\;
{\bm{W}^K}^\top\,\bm{W}^Q\,\bm{x}_c.
\]

\subsection*{Switching Linear Dynamical System}
\[
\mathrm{sim}\bigl(\bm{x}_{t+1}, \bm{x}_t; \bm{A}_k, \bm{b}_k\bigr)
\;=\;
\ln \pi_k
\;-\;
\tfrac12\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)^\top
\bm{\Sigma}_k^{-1}
\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]
\[
\frac{\partial}{\partial \bm{x}_{t+1}}\mathrm{sim}(\bm{x}_{t+1},\bm{x}_t;\bm{A}_k,\bm{b}_k)
\;=\;
\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr),
\]
\[
\frac{\partial}{\partial \bm{x}_t}\mathrm{sim}(\bm{x}_{t+1},\bm{x}_t;\bm{A}_k,\bm{b}_k)
\;=\;
-\,\bm{A}_k^\top \,\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr),
\]
\[
\frac{\partial}{\partial \bm{A}_k}\mathrm{sim}(\bm{x}_{t+1},\bm{x}_t;\bm{A}_k,\bm{b}_k)
\;=\;
\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr)\,\bm{x}_t^\top,
\]
\[
\frac{\partial}{\partial \bm{b}_k}\mathrm{sim}(\bm{x}_{t+1},\bm{x}_t;\bm{A}_k,\bm{b}_k)
\;=\;
\bm{\Sigma}_k^{-1}\,\bigl(\bm{x}_{t+1} - \bm{A}_k\,\bm{x}_t - \bm{b}_k\bigr).
\]

\subsection*{Predictive Coding}
\[
\mathrm{sim}\bigl(\bm{x}_i,\bm{\mu}_k\bigr)
\;=\;
-\,\tfrac12\,
\bigl\|\bm{x}_i \;-\; f_\phi(\bm{\mu}_k)\bigr\|^2.
\]
\[
\frac{\partial}{\partial \bm{x}_i}\,\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)
\;=\;
\bm{x}_i - f_\phi(\bm{\mu}_k),
\]
\[
\frac{\partial}{\partial \bm{\mu}_k}\,\mathrm{sim}(\bm{x}_i,\bm{\mu}_k)
\;=\;
\frac{\partial f_\phi(\bm{\mu}_k)}{\partial \bm{\mu}_k}
\;\bigl(\bm{x}_i - f_\phi(\bm{\mu}_k)\bigr).
\]


\end{document}
